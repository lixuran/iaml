{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)\n",
    "# Assignment 3 (Part A): Object Recognition [75%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "**It is important that you follow the instructions below to the letter - we will not be responsible for incorrect marking due to non-standard practices.**\n",
    "\n",
    "1. <font color='red'>We have split Assignment 3 into two parts to make it easier for you to work on them separately and for the markers to give you feedback. This is part A of Assignment 3 - Part B is the Mini-Challenge. Both Assignments together are still worth 50% of CourseWork 2. **Remember to submit both notebooks (you can submit them separately).**</font>\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/michael-camilleri/IAML2018) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. There are some questions which are **specific to those taking the Level-11 version** of the course (INFR11182 and INFR11152). These are clearly marked with the words **(LEVEL 11)** and must be completed by those taking the Level 11 course. Those on the Level 10 version (INFR10069) may (and are advised to) attempt such questions but this will not affect their mark in any way, nor will they get feedback on them.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation: **in questions where this is specified, you will be penalised if you go over.**\n",
    "\n",
    "1. Make sure to distinguish between **attributes** (columns of the data) and **features** (which typically refers only to the independent variables, i.e. excluding the target variables).\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. Marks *WILL* be deducted if the marker cannot understand your logic/results.\n",
    "\n",
    "1. **Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you must NOT borrow actual text or code from others. We ask that you provide a list of the people who you've had discussions with (if any). Please refer to the [Academic Misconduct](http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct) page for what consistutes a breach of the above.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "**IMPORTANT:** You must submit this assignment by **Thursday 15/11/2018 at 16:00**. \n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics is that normally you will not be allowed to submit coursework late. See the [ITO webpage](http://web.inf.ed.ac.uk/infweb/student-services/ito/admin/coursework-projects/late-coursework-extension-requests) for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Resubmission:** If you submit your file(s) again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline.\n",
    "\n",
    "**N.B.**: This Assignment requires submitting **two files (electronically as described below)**:\n",
    " 1. This Jupyter Notebook for Part A, *and*\n",
    " 1. The Jupyter Notebook for Part B\n",
    " \n",
    "All submissions happen electronically. To submit:\n",
    "\n",
    "1. Fill out this notebook (as well as Part B), making sure to:\n",
    "   1. save it with **all code/text and visualisations**: markers are NOT expected to run any cells,\n",
    "   1. keep the name of the file **UNCHANGED**, *and*\n",
    "   1. **keep the same structure**: retain the questions, **DO NOT** delete any cells and **avoid** adding unnecessary cells unless absolutely necessary, as this makes the job harder for the markers.\n",
    "\n",
    "1. Submit it using the `submit` functionality. To do this, you must be on a DICE environment. Open a Terminal, and:\n",
    "   1. **On-Campus Students**: navigate to the location of this notebook and execute the following command:\n",
    "   \n",
    "      ```submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb```\n",
    "      \n",
    "   1. **Distance Learners:** These instructions also apply to those students who work on their own computer. First you need to copy your work onto DICE (so that you can use the `submit` command). For this, you can use `scp` or `rsync` (you may need to install these yourself). You can copy files to `student.ssh.inf.ed.ac.uk`, then ssh into it in order to submit. The following is an example. Replace entries in `[square brackets]` with your specific details: i.e. if your student number is for example s1234567, then `[YOUR USERNAME]` becomes `s1234567`.\n",
    "   \n",
    "    ```\n",
    "    scp -r [FULL PATH TO 03_A_ObjectRecognition.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_A_ObjectRecognition.ipynb\n",
    "    scp -r [FULL PATH TO 03_B_MiniChallenge.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_B_MiniChallenge.ipynb\n",
    "    ssh [YOUR USERNAME]@student.ssh.inf.ed.ac.uk\n",
    "    ssh student.login\n",
    "    submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb\n",
    "    ```\n",
    "    \n",
    "   What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You should receive an automatic email confirmation after submission.\n",
    "  \n",
    "\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "The Level 10 and Level 11 points are marked out of different totals, however these are all normalised to 100%. Note that Part A (this notebook) is worth 75% of the total Mark for Assignment 3, while Part B (Mini-Challenge) is worth 25%: *keep this breakdown in mind when planning your work, especially for Part B*.\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n",
    "\n",
    "Note that while this is not a programming assignment, in questions which involve visualisation of results and/or long cold snippets, some marks may be deducted if the code is not adequately readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Use the cell below to include any imports you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s16/s1603859/miniconda3/envs/py3iaml/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# Nice Formatting within Jupyter Notebook\n",
    "%matplotlib inline\n",
    "from IPython.display import display # Allows multiple displays from a single code-cell\n",
    "\n",
    "# System functionality\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import Here any Additional modules you use. To import utilities we provide, use something like:\n",
    "#   from utils.plotter import plot_hinton\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from pandas.api.types import CategoricalDtype\n",
    "KNeighboursClassifier = KNeighborsClassifier # For the Brits! #????\n",
    "from __future__ import division\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the dataset\n",
    "In this assignment our goal is to recognize objects in images of realistic scenes. There are 19 different classes of object e.g. person, dog, cat, car, etc. The dataset derives from several thousands photographs harvested from the web. Each object of a relevant class has been manually annotated with a bounding box. Images can contain none, one or multiple objects of each class. We have prepared a [website](http://www.inf.ed.ac.uk/teaching/courses/iaml/2014/assts/asst3/images.html) where you can view the images.\n",
    "\n",
    "We are going to detect whether images contain a person or not - a binary classification problem. To save you time and to make the problem manageable with limited computational resources, we have preprocessed the dataset. We will use the [Bag of Visual Words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision) representation. Each column of the dataset (which is not a label), refers to a 'visual word'. Each image is represented by a 500 dimensional vector that contains the normalized count for each of 500 different visual words present in the respective image (a similar representation is used for the spambase dataset, just for real words). *Note that the normalisation procedure involves dividing the count vector by the total number of visual words in the image, i.e. the normalized counts sum to 1 for each image*. See the Appendix at the bottom of the notebook for more information. The image data is thus a $N \\times 500$ dimensional matrix where `N` is the number of images.\n",
    "\n",
    "The full dataset has 520 attributes (dimensions). The first attribute (`imgId`) contains the image ID which allows you to associate a data point with an actual image. The next 500 attributes (`dim1`, ..., `dim500`) are a normalized count vector for each visual word: these are the `features` of the data. The last 19 attributes, which follow the pattern `is_[class]` are the class labels -- here 1 means the class is present in the image. In most of the experiments (unless explicitly noted otherwise) you will only need the `is_person` attribute and the 500 dimensional feature vector. **Do not use the additional class indicator attributes as features** unless explicitly told to do so. \n",
    "\n",
    "**Important**: *Throughout the assignment you will be given various versions of the dataset that are relevant\n",
    "to a particular question. Please be careful to use the correct version of the dataset when instructed to do so.\n",
    "If you use the wrong version of the dataset by mistake no marks will be awarded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploration of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question1_1'></a>\n",
    "### ========== Question 1.1 --- [12 marks] ==========\n",
    "\n",
    "We will first get a feel for the data. *IMPORTANT: Show all your code!*\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Load the training dataset `Images_A_Train.csv` into a pandas dataframe, keeping only the Visual Features and the `is_person` column. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*Hint: You may wish to first have a look at the column names*<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using suitable pandas methods, summarise the key properties of the data, *and*<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] comment on your observations from ***(b)*** (dimensionality, data ranges, anything out of the ordinary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2093, 520)\n"
     ]
    }
   ],
   "source": [
    "# (a) # Your Code goes here:\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_A_Train.csv')\n",
    "A_train = pd.read_csv(data_path,delimiter=',')\n",
    "print(A_train.shape)\n",
    "A_train.head()\n",
    "A_train_selected = A_train.loc[:,\"dim1\":\"dim500\"]\n",
    "A_train_selected[\"is_person\"]=A_train[\"is_person\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.449116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.497523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.053329</td>\n",
       "      <td>0.010234</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.001751     0.000756     0.004317     0.001853     0.002272   \n",
       "std       0.001193     0.001406     0.003693     0.001340     0.001598   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000833     0.000000     0.001359     0.000756     0.001116   \n",
       "50%       0.001563     0.000340     0.003397     0.001698     0.002038   \n",
       "75%       0.002378     0.000893     0.006324     0.002717     0.003057   \n",
       "max       0.009851     0.016644     0.027514     0.010789     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.002220     0.001019     0.002446     0.002088     0.002746   \n",
       "std       0.001741     0.000873     0.002910     0.001971     0.002328   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000625     0.000744     0.001116   \n",
       "50%       0.001860     0.000744     0.001563     0.001698     0.002056   \n",
       "75%       0.003057     0.001488     0.003397     0.002717     0.003736   \n",
       "max       0.021739     0.005774     0.029830     0.028372     0.020380   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      ...          0.000563     0.004586     0.001541     0.003642   \n",
       "std       ...          0.001203     0.005825     0.001192     0.002657   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001698   \n",
       "50%       ...          0.000000     0.002717     0.001359     0.003057   \n",
       "75%       ...          0.000679     0.006454     0.002232     0.004808   \n",
       "max       ...          0.021739     0.053329     0.010234     0.024457   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.002200     0.002928     0.002173     0.002485     0.002978   \n",
       "std       0.001664     0.003254     0.001418     0.001997     0.002765   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000744     0.001116     0.001019     0.001019   \n",
       "50%       0.001860     0.001953     0.002038     0.002038     0.002232   \n",
       "75%       0.003057     0.003780     0.002976     0.003397     0.004076   \n",
       "max       0.011719     0.028125     0.008492     0.014509     0.028533   \n",
       "\n",
       "         is_person  \n",
       "count  2093.000000  \n",
       "mean      0.449116  \n",
       "std       0.497523  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim1      dim2      dim3      dim4      dim5      dim6      dim7  \\\n",
       "0  0.002232  0.000558  0.002790  0.000837  0.001674  0.001953  0.001395   \n",
       "1  0.001563  0.000391  0.007422  0.003516  0.003906  0.005078  0.001953   \n",
       "2  0.000521  0.000000  0.000000  0.001042  0.001563  0.005729  0.000521   \n",
       "3  0.002976  0.002232  0.004464  0.000372  0.000372  0.002232  0.000000   \n",
       "4  0.001359  0.000340  0.001359  0.000340  0.001359  0.002038  0.002378   \n",
       "5  0.000000  0.006324  0.000372  0.000372  0.000372  0.000372  0.000744   \n",
       "6  0.000340  0.000000  0.004416  0.000340  0.000679  0.006114  0.001359   \n",
       "7  0.000837  0.002232  0.000279  0.000279  0.000837  0.000000  0.000279   \n",
       "8  0.002378  0.001359  0.004755  0.001019  0.003736  0.001359  0.001019   \n",
       "9  0.001019  0.000340  0.006454  0.001698  0.001359  0.003736  0.000000   \n",
       "\n",
       "       dim8      dim9     dim10    ...        dim492    dim493    dim494  \\\n",
       "0  0.002232  0.003627  0.006138    ...      0.000558  0.005301  0.001116   \n",
       "1  0.002344  0.001953  0.001953    ...      0.000000  0.008203  0.001172   \n",
       "2  0.002083  0.003646  0.005208    ...      0.000000  0.000521  0.000521   \n",
       "3  0.003720  0.000000  0.002232    ...      0.000000  0.015253  0.000744   \n",
       "4  0.000000  0.003397  0.003397    ...      0.000679  0.000000  0.001359   \n",
       "5  0.008185  0.000372  0.000372    ...      0.005580  0.000744  0.000000   \n",
       "6  0.002717  0.003057  0.005435    ...      0.000000  0.003057  0.001359   \n",
       "7  0.006696  0.000000  0.000000    ...      0.002232  0.004185  0.000000   \n",
       "8  0.004076  0.003397  0.001698    ...      0.000679  0.007133  0.001359   \n",
       "9  0.004076  0.000000  0.003057    ...      0.000000  0.002717  0.002378   \n",
       "\n",
       "     dim495    dim496    dim497    dim498    dim499    dim500  is_person  \n",
       "0  0.004185  0.000837  0.006975  0.001953  0.001674  0.000558          1  \n",
       "1  0.007422  0.004297  0.001563  0.000000  0.003125  0.004687          0  \n",
       "2  0.002083  0.000000  0.000000  0.000521  0.003646  0.000000          0  \n",
       "3  0.001488  0.000744  0.000372  0.001860  0.000000  0.001860          1  \n",
       "4  0.001019  0.003736  0.008152  0.003736  0.000679  0.001698          0  \n",
       "5  0.000000  0.000000  0.000000  0.001116  0.000372  0.000372          0  \n",
       "6  0.001698  0.000679  0.006454  0.002378  0.002378  0.001019          1  \n",
       "7  0.000000  0.000837  0.000000  0.000279  0.000279  0.003627          0  \n",
       "8  0.002378  0.002378  0.003736  0.002038  0.003057  0.002378          1  \n",
       "9  0.004755  0.002378  0.000679  0.003057  0.001698  0.003057          0  \n",
       "\n",
       "[10 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2093 entries, 0 to 2092\n",
      "Columns: 501 entries, dim1 to is_person\n",
      "dtypes: float64(495), int64(6)\n",
      "memory usage: 8.0 MB\n"
     ]
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "display(A_train_selected.describe())\n",
    "display(A_train_selected.head(10))\n",
    "A_train_selected.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***\n",
    "the dataset consists of 2093 samples, each with 500 features and a classification label for existance of person in the image. the features value range from 0 to 1, with the mean of most features around 0.001-0.002.The mean and variance of each feature varies from one another. The minimal value of each feature is mostly 0, indicating that those features are missing in the picture. a small portion of the features are missing from 25% or more of the pictures, while the majority of them exist in over 75% of the pictures.there is a noticeable increase between the value of 75% datapoints and the maximum value of all datapoints for a lots of features, indicating that there exist a small portion of images that either have quite a few counts for those features, or doesn't have much features in it so each gets a higher value after normalisation. \n",
    "Also some features holds int64 value , this should be carefully dealt with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 --- [8 marks] ==========\n",
    "\n",
    "Now we will prepare the testing set in a similar manner.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Load the testing dataset `Images_A_Test.csv` into a pandas dataframe: again extract the Visual Features and the `is_person` column. <br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using similar methods to [Q1.1](#question1_1) verify that the testing set is similar to the training set.<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Indicate the dimensionality, and comment on any discrepancies if any (if they are similar, just say so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_A_Test.csv')\n",
    "A_test = pd.read_csv(data_path,delimiter=',')\n",
    "A_test_selected = A_test.loc[:,\"dim1\":\"dim500\"]\n",
    "A_test_selected[\"is_person\"]=A_test[\"is_person\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.473495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.499521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.042026</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.001744     0.000702     0.004720     0.001978     0.002321   \n",
       "std       0.001209     0.001364     0.003876     0.001417     0.001558   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000744     0.000000     0.001698     0.001019     0.001172   \n",
       "50%       0.001563     0.000340     0.003736     0.001698     0.002038   \n",
       "75%       0.002378     0.000758     0.007102     0.002734     0.003057   \n",
       "max       0.007133     0.022135     0.023438     0.008929     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002110     0.001037     0.002529     0.002006     0.002641   \n",
       "std       0.001559     0.000885     0.002736     0.001919     0.002293   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000679     0.000781     0.001019   \n",
       "50%       0.001803     0.000781     0.001698     0.001563     0.002038   \n",
       "75%       0.002976     0.001488     0.003397     0.002717     0.003397   \n",
       "max       0.010789     0.005757     0.022396     0.025000     0.013927   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      ...          0.000598     0.004817     0.001585     0.003611   \n",
       "std       ...          0.001241     0.005831     0.001243     0.002471   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001860   \n",
       "50%       ...          0.000000     0.002734     0.001359     0.003125   \n",
       "75%       ...          0.000679     0.006793     0.002232     0.004836   \n",
       "max       ...          0.012747     0.042026     0.009821     0.015625   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002196     0.002772     0.002239     0.002407     0.003097   \n",
       "std       0.001575     0.003182     0.001346     0.001868     0.002590   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000679     0.001250     0.001019     0.001359   \n",
       "50%       0.001860     0.001838     0.002038     0.002038     0.002604   \n",
       "75%       0.003057     0.003736     0.003057     0.003348     0.004092   \n",
       "max       0.013346     0.029225     0.007068     0.011889     0.026786   \n",
       "\n",
       "         is_person  \n",
       "count  1113.000000  \n",
       "mean      0.473495  \n",
       "std       0.499521  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.008285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim1      dim2      dim3      dim4      dim5      dim6      dim7  \\\n",
       "0  0.001698  0.000000  0.003057  0.002378  0.001019  0.001698  0.000340   \n",
       "1  0.002038  0.000000  0.004076  0.001019  0.001019  0.001019  0.000679   \n",
       "2  0.001116  0.000000  0.005208  0.001860  0.001116  0.000000  0.001488   \n",
       "3  0.001698  0.000340  0.004076  0.000679  0.002038  0.001019  0.002038   \n",
       "4  0.003736  0.001019  0.000679  0.001019  0.003736  0.002038  0.002038   \n",
       "5  0.002378  0.000000  0.003057  0.002717  0.001698  0.001698  0.001359   \n",
       "6  0.003057  0.000000  0.009511  0.001359  0.001698  0.000340  0.002378   \n",
       "7  0.000744  0.000372  0.001860  0.001860  0.002232  0.002232  0.002604   \n",
       "8  0.003057  0.000000  0.002717  0.005095  0.001698  0.002378  0.001019   \n",
       "9  0.002437  0.000000  0.007310  0.000975  0.000000  0.002924  0.000000   \n",
       "\n",
       "       dim8      dim9     dim10    ...        dim492    dim493    dim494  \\\n",
       "0  0.001019  0.001359  0.004416    ...      0.000000  0.002038  0.000340   \n",
       "1  0.001019  0.001019  0.002378    ...      0.000000  0.003736  0.003397   \n",
       "2  0.002232  0.001116  0.000372    ...      0.000000  0.014137  0.001116   \n",
       "3  0.001019  0.000340  0.004076    ...      0.000340  0.011889  0.001698   \n",
       "4  0.001359  0.002717  0.001698    ...      0.000679  0.000679  0.001359   \n",
       "5  0.002038  0.005095  0.002717    ...      0.000000  0.002378  0.001698   \n",
       "6  0.005095  0.002038  0.001019    ...      0.000340  0.003736  0.000000   \n",
       "7  0.000372  0.001860  0.004464    ...      0.000000  0.005952  0.003348   \n",
       "8  0.000000  0.001359  0.001359    ...      0.000000  0.005435  0.001359   \n",
       "9  0.000000  0.001949  0.008285    ...      0.000000  0.005848  0.002437   \n",
       "\n",
       "     dim495    dim496    dim497    dim498    dim499    dim500  is_person  \n",
       "0  0.003397  0.004416  0.000679  0.003736  0.005774  0.007812          1  \n",
       "1  0.005435  0.002038  0.003397  0.001019  0.001359  0.002717          1  \n",
       "2  0.004836  0.001488  0.000372  0.001116  0.001488  0.004092          1  \n",
       "3  0.001698  0.002378  0.002378  0.002038  0.001698  0.001698          1  \n",
       "4  0.001019  0.001019  0.001019  0.002378  0.001359  0.000000          1  \n",
       "5  0.003397  0.000679  0.002038  0.001698  0.001019  0.001019          1  \n",
       "6  0.002717  0.001019  0.001698  0.004755  0.001019  0.003736          1  \n",
       "7  0.001488  0.001488  0.002976  0.001860  0.002232  0.000372          1  \n",
       "8  0.001359  0.001359  0.003736  0.001359  0.001698  0.001019          1  \n",
       "9  0.000000  0.000487  0.003899  0.001949  0.005848  0.000975          1  \n",
       "\n",
       "[10 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1113 entries, 0 to 1112\n",
      "Columns: 501 entries, dim1 to is_person\n",
      "dtypes: float64(494), int64(7)\n",
      "memory usage: 4.3 MB\n"
     ]
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "display(A_test_selected.describe())\n",
    "display(A_test_selected.head(10))\n",
    "A_test_selected.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***\n",
    "there are 1113 rows/samples and 501 columns/attributes in the testing dataset. the testing dataset is similar to the training set, except for the fact that it has one more feature of type int64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [5 marks] ==========\n",
    "\n",
    "We will now prepare the data for training.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Split both the training and testing sets into a matrix of features (independent) variables [X_tr/X_tst] and a vector of prediction (dependent) variables [y_tr/y_tst]. ***[Optional]*** *As a sanity check, you may wish to verify the dimensionality of the X/y variables*.<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using seaborn's [countplot](https://seaborn.github.io/generated/seaborn.countplot.html?highlight=countplot#seaborn.countplot) function, visualise the distribution of the person-class (True/False) in the training and testing sets (use two figures or sub-plots). Annotate your figures.<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Do you envision any problems with the distribution under both sets? Would classification accuracy be a good metric for evaluating the performance of the classifiers? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X_tr is: (2093, 500)\n",
      "the shape of y_tr is: (2093,)\n",
      "the shape of X_tst is: (1113, 500)\n",
      "the shape of y_tst is: (1113,)\n"
     ]
    }
   ],
   "source": [
    "# (a) # Your Code goes here:\n",
    "X_tr = A_train_selected.drop(\"is_person\",axis=1)\n",
    "y_tr  = A_train_selected[\"is_person\"]\n",
    "X_tst = A_test_selected.drop(\"is_person\",axis=1)\n",
    "y_tst = A_test_selected[\"is_person\"]\n",
    "print(\"the shape of X_tr is: {}\".format(X_tr.shape))\n",
    "print(\"the shape of y_tr is: {}\".format(y_tr.shape))\n",
    "print(\"the shape of X_tst is: {}\".format(X_tst.shape))\n",
    "print(\"the shape of y_tst is: {}\".format(y_tst.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFUZJREFUeJzt3X+QZWV95/H3BwY0CCy40xqcAcewExUrEUhHUBKCkjLAJkKxIP5k1pCapIpYscrSsKmUWLpWwapk0TLUTsLvEBARddwyutZEJGAUZnAiCFJMGRc6IDMKgmCpGfzuH/f0cgd6enr63tv36Z73q+rWPee5z7nn21QPnz7Pfe5zUlVIktSavcZdgCRJMzGgJElNMqAkSU0yoCRJTTKgJElNMqAkSU0yoCRJTTKgJElNMqAkSU1aNu4CRmH58uW1atWqcZchSZrBpk2bflBVE7vqtyQDatWqVWzcuHHcZUiSZpDk/86ln0N8kqQmGVCSpCYtySG+YfmN91w17hK0yG368NnjLkFatLyCkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDVpZAGV5LIkW5Pc1df2/CRfTnJf93xw154kH0uyJcm3khzdd8yarv99SdaMql5JUltGeQV1BXDSM9rOAzZU1WpgQ7cPcDKwunusBS6BXqAB5wPHAK8Czp8ONUnS0jaygKqqm4FHntF8KnBlt30lcFpf+1XV83XgoCSHAL8HfLmqHqmqR4Ev8+zQkyQtQQv9GdQLq+ohgO75BV37CuCBvn5TXdvO2p8lydokG5Ns3LZt29ALlyQtrFYmSWSGtpql/dmNVeuqarKqJicmdnkfLElS4xY6oB7uhu7onrd27VPAoX39VgIPztIuSVriFjqg1gPTM/HWAJ/raz+7m813LPBYNwT4JeD1SQ7uJke8vmuTJC1xI7sfVJJrgROA5Umm6M3GuwC4Psk5wP3AmV33LwCnAFuAnwDvAKiqR5J8ELi96/eBqnrmxAtJ0hI0soCqqjfv5KUTZ+hbwLk7eZ/LgMuGWJq0x7r/A7827hK0iB32vjsX9HytTJKQJGkHBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJcwqoJBvm0iZJ0rDMeruNJM8F9qN3T6eDefoW7AcCLxpxbZKkPdiu7gf1x8C76IXRJp4OqMeBT4ywLknSHm7WgKqqi4GLk7yzqj6+QDVJkjS3O+pW1ceTvAZY1X9MVV01orokSXu4OQVUkquBw4HNwFNdcwEGlCRpJOYUUMAkcERV1SiLkSRp2ly/B3UX8MujLESSpH5zvYJaDtyd5DbgZ9ONVfWGkVQlSdrjzTWg3j/KIiRJeqa5zuL76qgLkSSp31xn8f2Y3qw9gH2BfYAnq+rAURUmSdqzzfUK6oD+/SSnAa8aSUWSJDHP1cyr6rPA64ZciyRJ/99ch/hO79vdi973ovxOlCRpZOY6i+8P+ra3A98DTh16NZIkdeb6GdQ7hnnSJN8Dfkxv2aTtVTWZ5PnAJ+mt9/c94I1V9WiSABcDpwA/Af5rVd0xzHokSe2Z6w0LVyb5TJKtSR5O8ukkKwc892ur6siqmuz2zwM2VNVqYEO3D3AysLp7rAUuGfC8kqRFYK6TJC4H1tO7L9QK4PNd2zCdClzZbV8JnNbXflX1fB04KMkhQz63JKkxcw2oiaq6vKq2d48rgIkBzlvA/0myKcnaru2FVfUQQPf8gq59BfBA37FTXdsOkqxNsjHJxm3btg1QmiSpBXMNqB8keVuSvbvH24AfDnDe46rqaHrDd+cmOX6Wvpmh7VkzCKtqXVVNVtXkxMQg2SlJasFcA+oPgTcC3wceAs4A5j1xoqoe7J63Ap+h96Xfh6eH7rrnrV33KeDQvsNXAg/O99ySpMVhrgH1QWBNVU1U1QvoBdb753PCJM9LcsD0NvB6erfzWA+s6bqtAT7Xba8Hzk7PscBj00OBkqSla67fg/r1qnp0eqeqHkly1DzP+ULgM73Z4ywD/r6qvpjkduD6JOcA9wNndv2/QG+K+RZ608yHOuVdktSmuQbUXkkOng6p7jtLcz12B1X1XeCVM7T/EDhxhvYCzp3PuSRJi9dcQ+ajwNeS3EBvgsIbgQ+NrCpJ0h5vritJXJVkI70FYgOcXlV3j7QySdIebc7DdF0gGUqSpAUxr9ttSJI0agaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJiyagkpyU5N4kW5KcN+56JEmjtSgCKsnewCeAk4EjgDcnOWK8VUmSRmlRBBTwKmBLVX23qn4OXAecOuaaJEkjtGzcBczRCuCBvv0p4Jj+DknWAmu73SeS3LtAte3plgM/GHcRrcpH1oy7BO0ef59nc36G9U4vnkunxRJQM/1XqR12qtYB6xamHE1LsrGqJsddhzQM/j63ZbEM8U0Bh/btrwQeHFMtkqQFsFgC6nZgdZKXJNkXeBOwfsw1SZJGaFEM8VXV9iR/CnwJ2Bu4rKq+Peay1OOwqpYSf58bkqradS9JkhbYYhnikyTtYQwoSVKTDCjNi0tPaSlJclmSrUnuGnctepoBpd3m0lNagq4AThp3EdqRAaX5cOkpLSlVdTPwyLjr0I4MKM3HTEtPrRhTLZKWKANK87HLpackaVAGlObDpackjZwBpflw6SlJI2dAabdV1XZgeumpe4DrXXpKi1mSa4F/Bl6aZCrJOeOuSS51JElqlFdQkqQmGVCSpCYZUJKkJhlQkqQmGVCSpCYZUJKkJhlQ0pAk+dq4a5CWEr8HJS1SSZZ1X5qWliSvoKQhSfJE93xIkpuTbE5yV5Lfnu2YJB9NckeSDUkmuvbDk3wxyaYk/5TkZV37FUkuSvIV4MIkv9OdZ3OSbyY5ID0f7s59Z5KzumNPSHJTkhuSfCfJNUlmWvhXasKycRcgLUFvAb5UVR/qbu643yx9nwfcUVXvTvI+4Hx6y0itA/6kqu5Lcgzw18DrumN+FfjdqnoqyeeBc6vq1iT7Az8FTgeOBF4JLAduT3Jzd+xRwCvoLe57K3AccMvQfnJpiAwoafhuBy5Lsg/w2araPEvfXwCf7Lb/DrixC5rXAJ/qu8B5Tt8xn6qqp7rtW4GLklwD3FhVU0l+C7i26/Nwkq8Cvwk8DtxWVVMASTYDqzCg1CiH+KQh6+7Oejzwb8DVSc7encPp/bv8UVUd2fd4eV+fJ/vOdQHwR8AvAV/vhgJnG7b7Wd/2U/hHqhpmQElDluTFwNaq+hvgUuDoWbrvBZzRbb8FuKWqHgf+NcmZ3fslySt3cq7Dq+rOqroQ2Ai8DLgZOCvJ3t1nWscDtw3jZ5MWkn89ScN3AvCeJP8OPAHMdgX1JPCKJJuAx4Czuva3Apck+UtgH+A64F9mOP5dSV5L72robuAfgJ8Dr+76F/Deqvr+9EQLabFwmrk0RkmeqKr9x12H1CKH+CRJTXKIT1oASb7BjjPxAN7u1ZO0cw7xSZKa5BCfJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUkGlCSpSQaUJKlJBpQkqUljCagkByW5Icl3ktyT5NVJnp/ky0nu654P7vomyceSbEnyrSSz3fxt+v2/OPqfQpI0H3P9f/S4VjO/GPhiVZ2RZF9gP+AvgA1VdUGS84DzgD8HTgZWd49jgEu655068MADf29yctJVcCWpTY/PpdOCr2ae5EB6d/r8leo7eZJ7gROq6qEkhwA3VdVLk/yvbvvaZ/bb2TkmJydr48aNo/1BJEnzkmRTVU3uqt84hvh+BdgGXJ7km0n+NsnzgBdOh073/IKu/wrggb7jp7q2HSRZm2Rjko3btm0b7U8gSRq5cQTUMuBo4JKqOgp4kt5w3s5khrZnXfZV1bqqmqyqyYmJieFUKkkam3F8BjUFTFXVN7r9G+gF1MNJDukb4tva1//QvuNXAg8uRKG/8Z6rFuI0WsI2ffjscZcgLVoLfgVVVd8HHkjy0q7pROBuYD2wpmtbA3yu214PnN3N5jsWeGy2z58kSUvDuGbxvRO4ppvB913gHfTC8vok5wD3A2d2fb8AnAJsAX7S9ZUkLXFjCaiq2gzMNIPjxBn6FnDuyIuSJDXFlSQkSU0yoCRJTTKgJElNMqAkSU0yoCRJTRrXNHNJY3D/B35t3CVoETvsfXcu6Pm8gpIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDVpbAGVZO8k30zyv7v9lyT5RpL7knwyyb5d+3O6/S3d66vGVbMkaeGM8wrqz4B7+vYvBP6qqlYDjwLndO3nAI9W1X8C/qrrJ0la4sYSUElWAv8Z+NtuP8DrgBu6LlcCp3Xbp3b7dK+f2PWXJC1h47qC+p/Ae4FfdPv/EfhRVW3v9qeAFd32CuABgO71x7r+O0iyNsnGJBu3bds2ytolSQtgwQMqye8DW6tqU3/zDF1rDq893VC1rqomq2pyYmJiCJVKksZp2RjOeRzwhiSnAM8FDqR3RXVQkmXdVdJK4MGu/xRwKDCVZBnwH4BHFr5sSdJCGugKKsmGubT1q6r/VlUrq2oV8CbgH6vqrcBXgDO6bmuAz3Xb67t9utf/saqedQUlSVpa5nUFleS5wH7A8iQH8/Qw3IHAi+ZZy58D1yX578A3gUu79kuBq5NsoXfl9KZ5vr8kaRGZ7xDfHwPvohdGm3g6oB4HPjHXN6mqm4Cbuu3vAq+aoc9PgTPnWackaZGaV0BV1cXAxUneWVUfH3JNkiQNNkmiqj6e5DXAqv73qqqrBqxLkrSHGyigklwNHA5sBp7qmgswoCRJAxl0mvkkcISz6iRJwzboF3XvAn55GIVIktRv0Cuo5cDdSW4DfjbdWFVvGPB9JUl7uEED6v3DKEKSpGcadBbfV4dViCRJ/Qadxfdjnl64dV9gH+DJqjpw0MIkSXu2Qa+gDujfT3IaM6wGIUnS7hrq7Taq6rP0bjwoSdJABh3iO71vdy9634vyO1GSpIENOovvD/q2twPfo3eLdkmSBjLoZ1DvGFYhkiT1G/SGhSuTfCbJ1iQPJ/l0kpXDKk6StOcadJLE5fTuePsiYAXw+a5NkqSBDBpQE1V1eVVt7x5XABNDqEuStIcbNKB+kORtSfbuHm8DfjiMwiRJe7ZBA+oPgTcC3wceAs4AnDghSRrYoNPMPwisqapHAZI8H/gIveCSJGneBr2C+vXpcAKoqkeAowZ8T0mSBg6ovZIcPL3TXUHNelWW5NAkX0lyT5JvJ/mz6WOTfDnJfd3zwV17knwsyZYk30py9IA1S5IWgUED6qPA15J8MMkHgK8B/2MXx2wH3l1VLweOBc5NcgRwHrChqlYDG7p9gJOB1d1jLXDJgDVLkhaBgQKqqq4C/gvwMLANOL2qrt7FMQ9V1R3d9o+Be+h9h+pU4Mqu25XAad32qcBV1fN14KAkhwxStySpfYNOkqCq7gbuns+xSVbR+8zqG8ALq+qh7j0fSvKCrtsK4IG+w6a6toee8V5r6V1hcdhhh82nHElSQ4Z6u43dkWR/4NPAu6rq8dm6ztD2rBXTq2pdVU1W1eTEhN8VlqTFbiwBlWQfeuF0TVXd2DU/PD101z1v7dqngEP7Dl8JPLhQtUqSxmPBAypJgEuBe6rqor6X1gNruu01wOf62s/uZvMdCzw2PRQoSVq6Bv4Mah6OA94O3Jlkc9f2F8AFwPVJzgHuB87sXvsCcAqwBfgJrlQhSXuEBQ+oqrqFmT9XAjhxhv4FnDvSoiRJzRnbJAlJkmZjQEmSmmRASZKaZEBJkppkQEmSmmRASZKaZEBJkppkQEmSmmRASZKaZEBJkppkQEmSmmRASZKaZEBJkppkQEmSmmRASZKaZEBJkppkQEmSmmRASZKaZEBJkppkQEmSmmRASZKatGgCKslJSe5NsiXJeeOuR5I0WosioJLsDXwCOBk4AnhzkiPGW5UkaZQWRUABrwK2VNV3q+rnwHXAqWOuSZI0QsvGXcAcrQAe6NufAo7p75BkLbC2230iyb0LVNuebjnwg3EX0ap8ZM24S9Du8fd5NudnWO/04rl0WiwBNdN/ldphp2odsG5hytG0JBuranLcdUjD4O9zWxbLEN8UcGjf/krgwTHVIklaAIsloG4HVid5SZJ9gTcB68dckyRphBbFEF9VbU/yp8CXgL2By6rq22MuSz0Oq2op8fe5IamqXfeSJGmBLZYhPknSHsaAkiQ1yYDSvLj0lJaSJJcl2ZrkrnHXoqcZUNptLj2lJegK4KRxF6EdGVCaD5ee0pJSVTcDj4y7Du3IgNJ8zLT01Iox1SJpiTKgNB+7XHpKkgZlQGk+XHpK0sgZUJoPl56SNHIGlHZbVW0Hppeeuge43qWntJgluRb4Z+ClSaaSnDPumuRSR5KkRnkFJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJQ1Jkq+NuwZpKfF7UNIilWRZ96VpaUnyCkoakiRPdM+HJLk5yeYkdyX57dmOSfLRJHck2ZBkoms/PMkXk2xK8k9JXta1X5HkoiRfAS5M8jvdeTYn+WaSA9Lz4e7cdyY5qzv2hCQ3JbkhyXeSXJNkpoV/pSYsG3cB0hL0FuBLVfWh7uaO+83S93nAHVX17iTvA86nt4zUOuBPquq+JMcAfw28rjvmV4HfraqnknweOLeqbk2yP/BT4HTgSOCVwHLg9iQ3d8ceBbyC3uK+twLHAbcM7SeXhsiAkobvduCyJPsAn62qzbP0/QXwyW7774Abu6B5DfCpvguc5/Qd86mqeqrbvhW4KMk1wI1VNZXkt4Bruz4PJ/kq8JvA48BtVTUFkGQzsAoDSo1yiE8asu7urMcD/wZcneTs3Tmc3r/LH1XVkX2Pl/f1ebLvXBcAfwT8EvD1bihwtmG7n/VtP4V/pKphBpQ0ZEleDGytqr8BLgWOnqX7XsAZ3fZbgFuq6nHgX5Oc2b1fkrxyJ+c6vKrurKoLgY3Ay4CbgbOS7N19pnU8cNswfjZpIfnXkzR8JwDvSfLvwBPAbFdQTwKvSLIJeAw4q2t/K3BJkr8E9gGuA/5lhuPfleS19K6G7gb+Afg58OqufwHvrarvT0+0kBYLp5lLY5Tkiaraf9x1SC1yiE+S1CSH+KQFkOQb7DgTD+DtXj1JO+cQnySpSQ7xSZKaZEBJkppkQEmSmmRASZKa9P8Ac/Gjnq831BsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "fig,ax=plt.subplots(2,1)\n",
    "plt.sca(ax[0])\n",
    "sns.countplot(y_tr)\n",
    "plt.sca(ax[1])\n",
    "sns.countplot(y_tst)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "np.sum(y_tst==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***\n",
    "the prior distributions of testing set are similar, and for training set there are more images without person than those with a person, but the ratio is still close to 1. So if we can taken into consideration the prior distribution during the training process the result should be improved. \n",
    "I think whether accuracy is good metric or not depends on the task we are trying to achieve.Assuming that the training and testing data are taken randomly from the total population of all such images, a higher accuracy would implies a high correctness overall, but it doesn't give any information about correctness on each part of the dataset, i.e. we don't know anything about precision and recall. So if we are just interested in the overall correctness, then we can use accuracy alone, otherwise we might consider combine it with someother metric or  change to some other criterion like r2 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Different Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 --- [3 marks] ==========\n",
    "\n",
    "As always, we wish to start with a very simple baseline classifier, which will provide a sanity check when training more advanced models.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Define a baseline classifier (indicate why you chose it/why it is relevant).<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Report the accuracy such a classifier would achieve on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***\n",
    "the baseline clasifier would be to simply classify each test sample as 0,i.e. doesn't contain person. This is both easy to implement and performs relatively better, as there are more pictures without person than with person in both training and testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of baseline classifier on the testing dataset is: 0.5265049415992812\n"
     ]
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "def baselineclassifier_predict(X_test):\n",
    "    return np.zeros((X_test.shape[0],1))\n",
    "print(\"the accuracy of baseline classifier on the testing dataset is: {}\".format(accuracy_score(y_tst,baselineclassifier_predict(X_tst))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 --- [9 marks] ==========\n",
    "<a id='question2_2'></a>\n",
    "Let us now train a more advanced Model.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Train a [`LogisticRegression`](http://scikit-learn.org/0.19/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier using default settings, except for the `solver` parameter which you should set to `lbfgs`. Report the classification accuracy score on the testing set.<br>\n",
    "&nbsp;&nbsp;**(b)** [Text] Comment on the performance of the Logistic Regressor in comparison with the baseline model.<br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Visualise the errors using an appropriate method to justify your answer to (b).<br>\n",
    "&nbsp;&nbsp;**(d)** [Text] Referring back to the observations in [Q1.1](#question1_1), and assuming that we know that the features should be informative, why do you think this may be happening?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy score of the logistic regression is 0.5265049415992812\n"
     ]
    }
   ],
   "source": [
    "# (a) # Your Code goes here:\n",
    "lrm = LogisticRegression(solver=\"lbfgs\")\n",
    "lrm.fit(X_tr,y_tr)\n",
    "print(\"the accuracy score of the logistic regression is {}\".format(lrm.score(X_tst,y_tst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ***Your answer goes here:*** \n",
    "the performance of the logistic regression model is the same as the baseline in terms of the accuracy score. This is either because they are the same model or they happen to classify the same amount of sample as its true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[586   0]\n",
      " [527   0]]\n",
      "[[586   0]\n",
      " [527   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEWCAYAAACg+rZnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGZRJREFUeJzt3XuUXWV9//H3Z4LcBGkhasgFEyEI/FCuRgVBWNwCAkmrQoBg8Zf+UqqgSMGiZoWLYq1tsVCjOFQEsZJEW2sKkctPYdFQ0AlCIwm3EITMJBDuIFTIzHz7x94TTg5zztknOWf23jOfF2uvnL33c579Hcj6zsN3P8/eigjMzKzYOvIOwMzMGnOyNjMrASdrM7MScLI2MysBJ2szsxJwsjYzKwEna9tskraR9B+SXpT0483o5zRJt7QytrxIOkTSQ3nHYcOHPM965JB0KnAusAfwMnAfcGlELNnMfk8HzgYOiojezQ604CQFMDkiVuYdi40cHlmPEJLOBf4R+BrwTmAX4NvAtBZ0/y7g4ZGQqLOQtEXeMdgwFBHehvkG7AD8HvhEnTZbkSTzNen2j8BW6bnDgG7gr4B1wFrgU+m5i4HXgfXpNWYBFwE/rOh7IhDAFun+GcAqktH9Y8BpFceXVHzvIKALeDH986CKc7cDXwHuTPu5BRhd42cbiP8LFfFPB44DHgaeA75U0X4KcBfwQtr2W8CW6bk70p/llfTnPbmi/78GngSuGziWfmfX9Br7p/tjgWeAw/L+u+GtPJtH1iPDh4CtgZ/WafNl4IPAvsA+JAlrTsX5MSRJfxxJQp4n6Y8j4kKS0fqCiNguIr5XLxBJbwWuAI6NiO1JEvJ9g7TbEbgxbbsTcBlwo6SdKpqdCnwKeAewJXBenUuPIfl3MA6YC1wFzAQOAA4B5kp6d9q2D/g8MJrk390RwKcBIuLQtM0+6c+7oKL/HUn+L2N25YUj4lGSRP4vkrYFvg9cExG314nXbCNO1iPDTsAzUb9McRpwSUSsi4inSUbMp1ecX5+eXx8Ri0lGle/ZxHj6gb0lbRMRayNi+SBtPgo8EhHXRURvRFwPPAicUNHm+xHxcET8D7CQ5BdNLetJ6vPrgfkkifjyiHg5vf5y4H0AEXFPRNydXvd3wHeBj2T4mS6MiNfSeDYSEVcBjwC/AnYm+eVolpmT9cjwLDC6QS11LPB4xf7j6bENfVQl+1eB7ZoNJCJeISkdnAmslXSjpD0yxDMQ07iK/SebiOfZiOhLPw8k06cqzv/PwPcl7S7pBklPSnqJ5P8cRtfpG+DpiPhDgzZXAXsD/xQRrzVoa7YRJ+uR4S7gDyR12lrWkPwv/IBd0mOb4hVg24r9MZUnI+LmiDiKZIT5IEkSaxTPQEw9mxhTM75DEtfkiHgb8CVADb5Td1qVpO1I7gN8D7goLfOYZeZkPQJExIskddp5kqZL2lbSWyQdK+kbabPrgTmS3i5pdNr+h5t4yfuAQyXtImkH4IsDJyS9U9KJae36NZJySt8gfSwGdpd0qqQtJJ0M7AXcsIkxNWN74CXg9+mo/y+rzj8FvPtN36rvcuCeiPhzklr8lZsdpY0oTtYjRERcRjLHeg7wNLAaOAv497TJV4GlwDLgt8Bv0mObcq1bgQVpX/ewcYLtIJlVsoZkhsRHSG/eVfXxLHB82vZZkpkcx0fEM5sSU5POI7l5+TLJqH9B1fmLgGslvSDppEadSZoGTCUp/UDy32F/Sae1LGIb9rwoxsysBDyyNjMrASdrM7MWk3S1pHWS7q9xXpKukLRS0jJJ+zfq08nazKz1riG5T1HLscDkdJtNMgOpLidrM7MWi4g7SG6g1zIN+EEk7gb+SNLO9fos7ANn1j+zync+7U22GXtI3iFYAfW+3tNoHnxDzeScLd++61+w8WMFOiOis4nLjSOZkTWgOz22ttYXCpuszcyKKk3MzSTnaoP9cqn7y8LJ2swMoH+wtVlt0w1MqNgfT4MVw65Zm5kB9PVm3zbfIuCT6ayQDwIvRkTNEgh4ZG1mBkBEf8v6knQ9yTPNR0vqBi4E3pJcJ64keZzCccBKkoeQfapRn07WZmYA/a1L1hFxSoPzAXymmT6drM3MAFo4sm4HJ2szMxjqG4xNc7I2MwOPrM3MyiBaM8ujbZyszcygpTcY28HJ2swMXAYxMysF32A0MysBj6zNzErANxjNzErANxjNzIovwjVrM7Pic83azKwEXAYxMysBj6zNzEqgb33eEdTlZG1mBi6DmJmVgssgZmYl4JG1mVkJOFmbmRVf+AajmVkJuGZtZlYCLoOYmZWAR9ZmZiXgkbWZWQl4ZG1mVgK9fvmAmVnxeWRtZlYCrlmbmZWAR9ZmZiXgkbWZWQl4ZG1mVgKeDWJmVgIReUdQl5O1mRm4Zm1mVgoFT9YdeQdgZlYI0Z99a0DSVEkPSVop6YJBzu8i6TZJ90paJum4Rn16ZG1mBtDX15JuJI0C5gFHAd1Al6RFEbGiotkcYGFEfEfSXsBiYGK9fp2szcyglWWQKcDKiFgFIGk+MA2oTNYBvC39vAOwplGnTtZmZtBUspY0G5hdcagzIjrTz+OA1RXnuoEPVHVxEXCLpLOBtwJHNrqmk7WZGTS1KCZNzJ01Tmuwr1TtnwJcExH/IOlDwHWS9o6oHYSTtZkZEP0tm2fdDUyo2B/Pm8scs4CpABFxl6StgdHAulqdejaImRkkZZCsW31dwGRJkyRtCcwAFlW1eQI4AkDSnsDWwNP1OvXI2swMWjYbJCJ6JZ0F3AyMAq6OiOWSLgGWRsQi4K+AqyR9nqREckZE/SWUTtZmZtDSRTERsZhkOl7lsbkVn1cABzfTp5O1mRl4BaNtujlfu4xDPzqD6TPPzDsUK5hjjj6M5fffwYMrlvCF8z+TdzjDQ0T2LQdO1gU2/bijuPKyr+YdhhVMR0cHV1x+KcefMJP37nM4J588nT33nJx3WOXXuhuMbdG2MoikPUhW7YwjKaCvARZFxAPtuuZwc+C+76Vn7VN5h2EFM+X9+/Hoo7/jsceeAGDhwp9x4gnH8MADj+QcWcm1bupeW7RlZC3pr4H5JJPDf00ylUXA9YM91MTMshs7bgyru9+Yttvds5axY8fkGNEw0deXfctBu0bWs4D/ExHrKw9KugxYDnx9sC9VLuH89j98lT//5CltCs+svKQ3L5BrMOvLMoiC32BsV7LuB8YCj1cd3zk9N6jKJZzrn1nlv31mg+jpXsuE8WM37I8ftzNrXS7bfAUvg7QrWZ8D/ELSI7zxQJNdgN2As9p0TbMRoWvpfey22yQmTpxAT8+TnHTSNE7/pGeEbLaR+MLciLhJ0u4kjwocR1Kv7ga6IiKfgk8JnX/h1+m6dxkvvPASR0yfyadnnc7HTjgm77AsZ319fXzunDksvvFHjOro4JprF7BixcN5h1V+BR9Zq6i1LpdBbDDbjD0k7xCsgHpf7xnsSXdNeWXujMw5562XzN/s6zXLKxjNzGBklkHMzEqn4GUQJ2szM0bu1D0zs3LxyNrMrAScrM3MSiCnZeRZOVmbmdHSdzC2hZO1mRm4DGJmVgqeDWJmVgIeWZuZlYCTtZlZ8UWfyyBmZsXnkbWZWfF56p6ZWRk4WZuZlUCxS9ZO1mZmANFb7GztZG1mBh5Zm5mVgW8wmpmVgUfWZmbF55G1mVkZeGRtZlZ80Zt3BPU5WZuZAVHwkXVH3gGYmRVCfxNbA5KmSnpI0kpJF9Roc5KkFZKWS/pRoz49sjYzo3Uja0mjgHnAUUA30CVpUUSsqGgzGfgicHBEPC/pHY36rZmsJb2t3hcj4qWswZuZFV0LyyBTgJURsQpA0nxgGrCios3/A+ZFxPMAEbGuUaf1RtbLgQBUcWxgP4BdmonezKzIok+NG6UkzQZmVxzqjIjO9PM4YHXFuW7gA1Vd7J72cycwCrgoIm6qd82ayToiJmSM28ys9JoZWaeJubPG6cGyfvUk7i2AycBhwHjgPyXtHREv1LpmphuMkmZI+lL6ebykA7J8z8ysLKJfmbcGuoHKwe54YM0gbX4WEesj4jHgIZLkXVPDZC3pW8DhwOnpoVeBKxt9z8ysTKI/+9ZAFzBZ0iRJWwIzgEVVbf6dJK8iaTRJWWRVvU6zzAY5KCL2l3QvQEQ8lwZgZjZsRGSvWdfvJ3olnQXcTFKPvjoilku6BFgaEYvSc0dLWgH0AedHxLP1+s2SrNdL6iCtuUjaicIvzDQza04rF8VExGJgcdWxuRWfAzg33TLJkqznAf8KvF3SxcBJwMVZL2BmVgb9TcwGyUPDZB0RP5B0D3BkeugTEXF/e8MyMxtaGW4c5irrCsZRwHqSUoiXqJvZsFP0ZJ1lNsiXgeuBsSRTUH4k6YvtDszMbChFZN/ykGVkPRM4ICJeBZB0KXAP8DftDMzMbCgVfWSdJVk/XtVuCxrMBzQzK5tWTd1rl3oPcvomSY36VWC5pJvT/aOBJUMTnpnZ0Ogr8WyQgRkfy4EbK47f3b5wzMzyUdqRdUR8bygDMTPLU+lr1pJ2BS4F9gK2HjgeEbu3MS4zsyGV1yyPrLLMmb4G+D7JY/+OBRYC89sYk5nZkGvhU/faIkuy3jYibgaIiEcjYg7p06LMzIaLvv6OzFseskzde02SgEclnQn0AA3fF2ZmViZFL4NkSdafB7YDPktSu94B+L/tDMrMbKj1l3U2yICI+FX68WXeeAGBmdmwUtqpe5J+ypvfG7ZBRPxpWyIyM8tBmcsg3xqyKMzMclbaMkhE/GIoAzEzy1Neszyyyvo8azOzYa3gVRAnazMzKHEZpJqkrSLitXYGY2aWl6LPBsnyppgpkn4LPJLu7yPpn9oemZnZEOpvYstDlor6FcDxwLMAEfHfeLm5mQ0zgTJvechSBumIiMeTFecb9LUpHjOzXPQWvAySJVmvljQFCEmjgLOBh9sblpnZ0MprxJxVlmT9lySlkF2Ap4D/nx4zMxs28qpFZ5Xl2SDrgBlDEIuZWW5KP7KWdBWDzBePiNlticjMLAelH1mTlD0GbA38CbC6PeGYmeWjr+wj64hYULkv6Trg1rZFZGaWg4K/L3eTlptPAt7V6kDMzPLUX/aRtaTneaNm3QE8B1zQzqDMzIZaqR/klL57cR+S9y4C9EcU/RHdZmbNK/oNxrrLzdPE/NOI6Es3J2ozG5b6pcxbHrI8G+TXkvZveyRmZjnqa2LLQ81kLWmgRPJhkoT9kKTfSLpX0m+GJjwzs6HRr+xbI5KmpjlzpaSa9/gkfVxSSDqwUZ/1ata/BvYHpjcOzcys3Fo1GyR9htI84CigG+iStCgiVlS12x74LPCrLP3WS9YCiIhHNyliM7MSaeENuSnAyohYBSBpPjANWFHV7ivAN4DzsnRaL1m/XdK5tU5GxGVZLmBmVgbNLIqRNBuofORGZ0R0pp/HsfEq727gA1Xf3w+YEBE3SNrsZD0K2A4KPlPczKwFmpm6lybmzhqnB8uZGwbukjqAbwJnNHHJusl6bURc0kxnZmZl1de6YWk3MKFifzywpmJ/e2Bv4Pb0pS5jgEWSToyIpbU6bVizNjMbCVq4KKYLmCxpEsmCwhnAqQMnI+JFYPTAvqTbgfPqJWqoP8/6iM2J1sysTFr1wtyI6AXOAm4GHgAWRsRySZdIOnFT46s5so6I5za1UzOzsmnlKxgjYjGwuOrY3BptD8vS56Y8dc/MbNgp+rNBnKzNzMhvGXlWTtZmZgzPlw+YmQ07LoOYmZWAk7WZWQkU/WH9TtZmZrhmbWZWCp4NYmZWAv0FL4Q4WZuZ4RuMZmalUOxxtZO1mRngkbWZWSn0qthjaydrMzNcBjEzKwWXQczMSsBT98zMSqDYqdrJ2swMcBnEzKwU+go+tnayNjPDI2szs1IIj6zNzIqv6CPrjrwDsNrmfO0yDv3oDKbPPDPvUKxgjjn6MJbffwcPrljCF87/TN7hDAv9ROYtD07WBTb9uKO48rKv5h2GFUxHRwdXXH4px58wk/fuczgnnzydPfecnHdYpRdNbHlwsi6wA/d9Lzu8bfu8w7CCmfL+/Xj00d/x2GNPsH79ehYu/BknnnBM3mGVXi+RecuDk7VZyYwdN4bV3Ws27Hf3rGXs2DE5RjQ8RBP/5GHIk7WkT9U5N1vSUklL//kH1w9lWGalIb35ZYERxZ7JUAb9TWx5yGM2yMXA9wc7ERGdQCfA+mdW+W+f2SB6utcyYfzYDfvjx+3M2rVP5RjR8DAip+5JWlbrFPDOdlzTbKToWnofu+02iYkTJ9DT8yQnnTSN0z/pGSGbq+hT99o1sn4ncAzwfNVxAf/VpmsOO+df+HW67l3GCy+8xBHTZ/LpWafzMd9IGvH6+vr43DlzWHzjjxjV0cE11y5gxYqH8w6r9PoKXkpqV7K+AdguIu6rPiHp9jZdc9j5u4svyDsEK6if3/RLfn7TL/MOY1gZkY9IjYhZdc6d2o5rmpltjhFZszYzK5uRWrM2MyuVopdBvCjGzIzWLoqRNFXSQ5JWSnrTzSdJ50paIWmZpF9IelejPp2szcxIZoNk3eqRNAqYBxwL7AWcImmvqmb3AgdGxPuAnwDfaBSfk7WZGS196t4UYGVErIqI14H5wLTKBhFxW0S8mu7eDYxv1KmTtZkZzS03r3w0RrrNruhqHLC6Yr87PVbLLODnjeLzDUYzM5qbulf5aIxBvPnhLTWerCppJnAg8JFG13SyNjOjpbNBuoEJFfvjgTXVjSQdCXwZ+EhEvNaoUydrMzNa+uTCLmCypElADzAD2GgxoKT9gO8CUyNiXZZOnazNzIC+Fo2sI6JX0lnAzcAo4OqIWC7pEmBpRCwC/g7YDvhx+sjbJyLixHr9OlmbmdHaRTERsRhYXHVsbsXnI5vt08nazIziv8DBydrMjOIvN3eyNjPDT90zMyuFkfryATOzUnEZxMysBJyszcxKwLNBzMxKwCNrM7MS8GwQM7MS6Itiv4XRydrMDNeszcxKwTVrM7MScM3azKwE+l0GMTMrPo+szcxKwLNBzMxKwGUQM7MScBnEzKwEPLI2MysBj6zNzEqgL/ryDqEuJ2szM7zc3MysFLzc3MysBDyyNjMrAc8GMTMrAc8GMTMrAS83NzMrAdeszcxKwDVrM7MS8MjazKwEPM/azKwEPLI2MysBzwYxMysB32A0MyuBopdBOvIOwMysCKKJfxqRNFXSQ5JWSrpgkPNbSVqQnv+VpImN+nSyNjMjGVln3eqRNAqYBxwL7AWcImmvqmazgOcjYjfgm8DfNorPydrMjKRmnXVrYAqwMiJWRcTrwHxgWlWbacC16eefAEdIUr1OC1uzfsvod9cNfCSRNDsiOvOOowh6X+/JO4TC8N+L1up9vSdzzpE0G5hdcaiz4r/FOGB1xblu4ANVXWxoExG9kl4EdgKeqXVNj6zLYXbjJjYC+e9FTiKiMyIOrNgqf2kOlvSrh+NZ2mzEydrMrLW6gQkV++OBNbXaSNoC2AF4rl6nTtZmZq3VBUyWNEnSlsAMYFFVm0XAn6WfPw78MhrcuSxszdo24rqkDcZ/LwoorUGfBdwMjAKujojlki4BlkbEIuB7wHWSVpKMqGc06ldFnwhuZmYug5iZlYKTtZlZCThZF1yjZas28ki6WtI6SffnHYsNHSfrAsu4bNVGnmuAqXkHYUPLybrYsixbtREmIu6gwZxcG36crIttsGWr43KKxcxy5GRdbE0vSTWz4cnJutiyLFs1sxHAybrYsixbNbMRwMm6wCKiFxhYtvoAsDAilucbleVN0vXAXcB7JHVLmpV3TNZ+Xm5uZlYCHlmbmZWAk7WZWQk4WZuZlYCTtZlZCThZm5mVgJO1DSlJv0//HCvpJw3aniNp2yb7P0zSDVmPV7U5Q9K3mrze7ySNbuY7ZpvCydo2W/p0wKZExJqI+HiDZucATSVrs+HKydpqkjRR0oOSrpW0TNJPBka66YhyrqQlwCck7SrpJkn3SPpPSXuk7SZJuktSl6SvVPV9f/p5lKS/l/Tb9DpnS/osMBa4TdJtabuj075+I+nHkrZLj09N41wC/GmGn2uKpP+SdG/653sqTk9If46HJF1Y8Z2Zkn4t6T5J392UX1Bmm8PJ2hp5D9AZEe8DXgI+XXHuDxHx4YiYT/Ly1rMj4gDgPODbaZvLge9ExPuBJ2tcYzYwCdgvvc6/RMQVJM9BOTwiDk9LDXOAIyNif2ApcK6krYGrgBOAQ4AxGX6mB4FDI2I/YC7wtYpzU4DTgH1JfgkdKGlP4GTg4IjYF+hL25gNGb/d3BpZHRF3pp9/CHwW+Pt0fwFAOsI9CPixtOFBgVulfx4MfCz9fB3wt4Nc40jgynR5PREx2LOaP0jyAoY702tsSbLkeg/gsYh4JI3lhyTJv54dgGslTSZ5iuFbKs7dGhHPpn39G/BhoBc4AOhKr70NsK7BNcxaysnaGql+HkHl/ivpnx3AC+moM0sf1ZSxza0RccpGB6V9M3y32leA2yLiTyRNBG6vODfYzyvg2oj4YpPXMWsZl0GskV0kfSj9fAqwpLpBRLwEPCbpEwBK7JOevpPkaYFQu3RwC3CmpC3S7++YHn8Z2D79fDdwsKTd0jbbStqdpKQxSdKuFTE2sgPQk34+o+rcUZJ2lLQNMD2N/xfAxyW9YyA+Se/KcB2zlnGytkYeAP5M0jJgR+A7NdqdBsyS9N/Act54/djngM9I6iJJkoP5Z+AJYFn6/VPT453AzyXdFhFPkyTW69NY7gb2iIg/kJQ9bkxvMD6e4Wf6BvA3ku4Eqm8ULiEp19wH/GtELI2IFST18lvSa98K7JzhOmYt46fuWU1pieCGiNg751DMRjyPrM3MSsAjazOzEvDI2sysBJyszcxKwMnazKwEnKzNzErAydrMrAT+F7kxYXoeq4ceAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (c) # Your Code goes here:\n",
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True)\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel(\"predicted label\")\n",
    "plt.figure()\n",
    "cm1=confusion_matrix(y_tst,lrm.predict(X_tst))\n",
    "print(cm1)\n",
    "cm1_norm = cm1/cm1.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm1_norm,classes=lrm.classes_)\n",
    "plt.figure()\n",
    "cm2=confusion_matrix(y_tst,baselineclassifier_predict(X_tst))\n",
    "print(cm2)\n",
    "cm2_norm = cm2/cm2.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm2_norm,classes=lrm.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) ***Your answer goes here:*** #attention to this question!\n",
    "from Q1.1 we see that the maximum number of the features are significantly larger than the mean of the feature,i.e. the distribution of the features are skewed. The mean and variance of each feature are also not unified, which means feature with larger mean gets more importance. those outliers might have a effect on the classification boundary as logistic regression is a discriminative method that might be affected by those outliers. another possible explanation is that the dataset is not linear seperatable,which might leads the logistic regression to classify every sample into one single class,and that be a result from the fact that the dataset is sparse(have lots of zeros). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 --- [13 marks] ==========\n",
    "\n",
    "You should have noticed that the performance of the above logistic regressor is less than satisfactory. Let us attempt to fix this by preprocessing the inputs `X`.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Before applying the processing, comment on whether you should base any parameters of the preprocessing on the training or testing set or both and what repurcussions this may have.<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Following from your observations in [Q2.2.(d)](#question2_2), process the features in both the **training** as well as the **testing** sets accordingly. *Hint: There is an sklearn [package](http://scikit-learn.org/0.19/modules/preprocessing.html) which may be very useful.* <br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Now Train a Logistic Regressor on the transformed training set, keeping the same settings as in the previous question. Report the classification accuracy on the testing set and visualise the errors in a similar way to [Q2.2(c)](#question2_2). <br>\n",
    "&nbsp;&nbsp;**(d)** [Text] Finally comment on the comparative performance with [Q2.2](#question2_2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:*** does repurcussion always means negative effect?\n",
    "This depends on the processing we are trying to perform. For this question, I believe we are supposed to perform a nonlinear transformation on the dataset to correct the skewed features. for this purpose, we should apply the preprocessing to both the training and the testing set. such an action might or might not improve the performance, depending on which transformation we would choose. \n",
    "\n",
    "On the other hands,if the preprocessing is for removing outliers , then generally speaking that needs to be applied to both the training dataset, but not neccesary the testing datatset(depends on whether the outliers are systematic or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:\n",
    "from sklearn import preprocessing\n",
    "X_tr_scaled = preprocessing.scale(X_tr)\n",
    "X_tst_scaled = preprocessing.scale(X_tst)\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "X_tr_trans = quantile_transformer.fit_transform(X_tr)\n",
    "X_tst_trans = quantile_transformer.transform(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of testing data after normalisation: 0.6424079065588499\n",
      "accuracy of testing data after transformation: 0.6343216531895777\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEWCAYAAACg+rZnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmcHFW9/vHPM5OdLJCwZmERgmwCCTFsgqiAICKIqGwKCkauAiLCT7hyUSOI+ENQLly4QbgIsqNCxLCE7SIYIAQIkCAQw5KQkJAdyEJm5nv/qJrQ00wvk0wvlTzvvOqVqjqnTp2urv72mVOnqhURmJlZfWuodQXMzKw0B2szswxwsDYzywAHazOzDHCwNjPLAAdrM7MMWKeCtaRHJJ2Uzh8r6f4K7CMkbZOzfKCkOzt7PznlXyfp/Gpvax1XzeMt6R5Jx1djX51B0lOSduxA/i3Tz1qXStarnqxTwTpXRNwYEQdWYVe/BH5Vhf10KkknSHqs1vWoBEnHSHpD0vuS7pTUv9Z16mwRcXBE/KHW9WglaSdJ90maJ6m9mzsuBkZXaN/7SZpZibKrqW6D9drwjSnpk0C/iHii1nWxRNp6+2/gG8AmwFLgv2paqU6kRD1+rlcCtwEnFkgfC3xG0mbVq1K2dPqbKul1SWdKel7SYkm3SuqRk/4dSdMkLZA0VtLAnLSQ9H1JrwKv5qz7nqRXJb0r6ReStpY0QdISSbdJ6pbm3UDS3ZLekbQwnR9coJ6rWo7pCX6ppLlpnZ+XtFOa1l3SxZLelDRH0lWSeuaUc5ak2ZJmSfp23m4OBv43J2+x/fSU9Ju0xbdY0mOt+5F0u6S30/WPFvtzUdIXJT0naZGkf0jaOSdtmKRn0uN4K9CjQBnbA1cBe0p6T9KidH0/Sdenx/cNSecWCgySfpbW+4/p/l6QtK2kc9LXP0PSgTn5+0m6Jj2Wb0k6X1Jjmra1pIckzU9bZjdKWj9n26LnXJ5jgb9GxKMR8R7wH8ARkvoUeB2/S+u6RNIkSfvkvcbb0mPyrqQpkkasxvHunr5fO+Ws20jSMkkblzqvlXTvXSDpcZIvn4+pbZffGh0/SYel59QSSf+SdFCp9yxfRLwcEdcAUwqkLwcmAe3+tSupUcnncJ6k6cAheenfkvRSeqynS/puun494B5gYHouvydpoKSRSmLIorT+lyuNI3UrIjp1Al4HngIGAv2Bl4CT07TPAvOA4UB34D+BR3O2DWB8ul3PnHVjgb7AjsAK4EHgY0A/YCpwfJp3APAVoBfQB7gduDOn/EeAk9L5E4DH0vnPk5wo6wMCtgc2S9N+m+6/f1rmX4EL07SDgDnATsB6wE1pfbdJ028HzsrZf7H9XJHWbxDQCOwFdE/Tvp3uu3tan+dyyrwOOD+dHw7MBXZPyzg+fT+6A92AN4AfAl2BI0laO+cXeB9XHZ+cddcDd6V12RJ4BTixwPY/A5anr7lLuu1rwE/S/X8HeC0n/50kLd71gI1JzqHvpmnbAAekr2Mj4FHgt+Wcc+3U6y7gx3nr3gN2K5D/OJLzqgvwI+BtoEfea/xCerwvBJ5I0zp6vK8FLshZ/j5wbwfO6zdJPh9d0v09wofn+mofP2AksDjdvoHk/Nyu1HtWJD5sA0SBtMuASwqknQz8ExiS1vFhks9alzT9EGBrks/Vp0m+tIanafsBM/PK2w3YIz1eW6av+fTOjoedOVUqWB+Xs/xr4Kp0/hrg1zlpvdMTeMt0OYDP5pUXwN45y5PI+bABv8k98fK23RVYmHdStxesP0sSePYAGnLyC3gf2Dpn3Z6kQYbkA/arnLRtaRusx5MTNIrspwFYBuxSxvFdP91Hv3T5Oj4M1lcCv8jL/3J68u4LzAKUk/YPygzWJMFoBbBDzrrvAo8U2P5nwPic5UNJgmJjutwnfR3rk3RHrCD9gk7TjwYeLlD24cCz5Zxz7Wz7IHmBHHgL2K/M83th6/uUvsYHctJ2AJal8x093vsD03OWHwe+2YHzenRenkdIz/U1OX4kwfjSdsro0HuWk6dYsL4AuLZA2kO0/SwdSE6wbif/ncAP0vn9yAvW7eQ/HfhLOedAraZK9Qu/nTO/lOQbm/T/Z1oTIuI9SfNJvq1fT1fPaKe8OTnzy9pZ3hRAUi/gUpIW7wZpeh9JjRHRXKiyEfGQpMtJWrebS/oLcCbJn629gEmSWrOLJHC1vp5JOUW9kVf0QpKgVM5+egD/yq9b+mflBcBXSVpFLWnShiQtnlxbAMdLOjVnXbe0ngG8FemZWaC+xWzIh63F3O0HFdkm/32al/M+LEv/753WryswO+c4N5CeC5I2Jml17UNyPBtIjm2uQudcvvdI/krL1Rd4t73Mkn4EnMSHx7AvybEotN8eSq63DKRjx/shoKek3dMydwX+ktahnPO6vc9N62tYk+M3BBjXTrFbUOQ9W019gEUF0gbmld3mWEo6GPgpSYOpgeRz+0KhHUnaFrgEGJHm7ULbz3LdqfaFiFkkbzKwqj9pAEnLptWaPAbwR8DHgd0joi9J6waSAFtURFwWEbuR/Cm5LXAWSZfNMmDHiFg/nfpFRO90s9kkJ3OrzfOKfT4tq5z9LCf5My7fMcBhJC2vfiR/shV6TTNI/pReP2fqFRE3p3UdpJxPVjv1bVPVvOV5JH8FbZGzbnPavnerawZJK23DnHr3jYjWvvkL0/rsnL6vx1HGe1rAFGCX1gVJHyPpHnglP2PaP/1j4GvABhGxPskXZDn77tDxjogWkgtwR5O853dHROsXSDnndbHPzZocvxm0f16Wes9Wx/bA5AJpBT9rkroDfyIZUbJJ+j6N48PX2N6xuZKkW2Voekz+ndU/p6qi2sH6JuBbknZND/AvgScj4vVOKr8PSXBdpGQ41k/L2UjSJyXtLqkrSbfHcqA5/QBdDVyatk6QNEjS59NNbwNOkLRD2vrJ3984ki6IcvZzLXBJevGjUdKe6THqQ/KhmE/SAvhlkZdyNXByug9JWk/SIUounk0AmoDTJHWRdARJf2Qhc4DBrRdd0hbcbcAFkvpI2gI4A/hjkTLKEhGzgfuB30jqK6khvSjWeuz6kLSIF0kaRPIFt7puBA6VtE/aWBgN/DknMObqQ3LM3gG6SDqPj7bKC+no8Ybk8/F1kougN+XVo8Pndd72q3v8riH5zH4ufV8GSdqujPesjfR87EHy1xmSeqTnd2t6d5J+5PEF6nEbybEcLGkD4OyctG4kX7jvAE1pKzv3QuUcYICkfjnr+gBLgPckbQf8W7kHpFaqGqwj4kGSq+9/Ivmm3Bo4qhN38VugJ0kr8Ang3jK360sS6BaS/Hk1n+RbGpKW1TTgCUlLgAdIWjlExD3pPh9K8zyUW2hEPAMsTv+0LbWfM0n+bJsILAAuInl/rk/zvkVyMbXgMMCIeJrkwt3l6T6mkfQ9ExEfAEekywtJgsKfixyTh0haoW9LmpeuO5XkS2Y68BhJQLm2SBkd8U2SD93UtH53AK3DuH5OcvF0MfC3EvUuKiKmkFysupHkYmwf4HsFst9HMpLgFZL3YDll/pm/GsebiHiS5PgOTPfbanXP61arffwi4ingWyTdMItJRje1/nVV7D3LtwXJF07raJBlJNdTWn2J5PrHrALbX03yfkwm6Upd9RrSL9rTSAL6QpK/TMbmpP8TuBmYno7+GEjyeTuGpPvrauDWIoehLqhtl5p1NiXD074XEYfXui5m9UrSkyQji16sdV3qlYO1mVkG1OOdTmZmmSbpWiU3f7X7l0Lah3+ZkhsEn5c0vFSZDtZmZp3vOpKhloUcDAxNp1Eko1OKcrA2M+tkEfEoyUCBQg4Dro/EE8D6KvFclLp9WNK7px/qznT7iPPv7F06k61zLnr95jUeI71y3vSyY063jbb+LkmLuNWYiBjTgd0Nou3IopnputmFNqjbYG1mVq/SwNyR4JyvvS+Xol8WDtZmZgAtBZ9IUQkzaXtH5mCSO7wLcp+1mRlAc1P505obC3wzHRWyB7A4vSu0ILeszcyA5KkPnUPSzSRP+9tQya/U/JTkwVdExFUkj6L4AsldxktJ7hItysHazAygpfOCdUQcXSI9SJ5ZXjYHazMzgE5sWVeCg7WZGVT7AmOHOVibmYFb1mZmWRCdM8qjYhyszcygUy8wVoKDtZkZuBvEzCwTfIHRzCwD3LI2M8sAX2A0M8sAX2A0M6t/Ee6zNjOrf+6zNjPLAHeDmJllgFvWZmYZ0Lyy1jUoysHazAzcDWJmlgnuBjEzywC3rM3MMsDB2sys/oUvMJqZZYD7rM3MMsDdIGZmGeCWtZlZBrhlbWaWAW5Zm5llQJN/fMDMrP65ZW1mlgHuszYzywC3rM3MMsAtazOzDHDL2swsAzwaxMwsAyJqXYOiHKzNzMB91mZmmVDnwbqh1hUwM6sL0VL+VIKkgyS9LGmapLPbSd9c0sOSnpX0vKQvlCrTLWszM4Dm5k4pRlIjcAVwADATmChpbERMzcl2LnBbRFwpaQdgHLBlsXIdrM3MoDO7QUYC0yJiOoCkW4DDgNxgHUDfdL4fMKtUoQ7WZmbQoWAtaRQwKmfVmIgYk84PAmbkpM0Eds8r4mfA/ZJOBdYD9i+1TwdrMzPo0E0xaWAeUyBZ7W2St3w0cF1E/EbSnsANknaKKFwJB2szMyBaOm2c9UxgSM7yYD7azXEicBBAREyQ1APYEJhbqFCPBjEzg6QbpNypuInAUElbSeoGHAWMzcvzJvA5AEnbAz2Ad4oV6pa1mRl02miQiGiSdApwH9AIXBsRUySNBp6OiLHAj4CrJf2QpIvkhIjit1A6WJuZQafeFBMR40iG4+WuOy9nfiqwd0fKdLA2M4O6v4PRwbqONG43nB5HfAfUwMonxvPBg3e0Se9++Ek0Dv0EAOraHfXpx3vnHF2LqloVbfvpXfjSed9EjQ1MvPVhHrmybffn7sfuz57fOIBoaWHF+8v58zm/Z+60t2pU2wzzg5ysLGqgx5Ens/TK/yAWzafXGZfQ9OKTtMz5cLjmijt/v2q+6z5fpHHwx2pRU6siNYjDR3+L3x/3Sxa/PZ9Txl7A1PGT2gTj5+56nCdvfACA7fffjS/+xze49vhf1arK2bWutqwlbUdy184gkg70WcDYiHipUvvMsoYthtIybzYxfw4ATc8+SpdP7M4Hc2a0m7/r8H1Zcc9N1ayi1cCQXbdh/htvs2BGMqJr8l8nsMOBI9oE6xXvLVs1361X97pvIdatzhu6VxEVCdaSfkwy6PsW4Kl09WDgZkm3RIS/9vM09BtAy8J5q5ZbFs2ncYtt282rDTZC/Teh+dXnq1U9q5F+m2zAolnzVy0vnj2fzXfd5iP59vzGAexz0iE0du3CmGPOr2YV1x6dNBqkUirVsj4R2DEiVuaulHQJMAVoN1jn3sL5u89+gm99YosKVa8etXPTU4EWUtfh+9I0+fG6/xki6wT66HnR3mkx4YbxTLhhPLt+aS8+d+qXue1HV1ahcmuXqPNukErdFNMCDGxn/WZpWrsiYkxEjIiIEetWoIaWxfNo2GDDVcsN6w8glixoN2+XYfuw8plHq1U1q6HFby9g/YEDVi3322wAS+YuLJh/8l8nsOMBI6pRtbVPS5Q/1UClgvXpwIOS7pE0Jp3uBR4EflChfWZay5uv0rDhQNR/E2jsQpdh+9L04lMfyaeNB6FevWl5/Z81qKVV28zJ/2LAlpuyweCNaOzayC6H7slL4ye1yTNgy01XzW/32WHMe/3taldz7dCJz7OuhIp0g0TEvZK2JXlU4CCSv/FnAhMjor47hmqlpYXlf7qKXif/HBoaWPnkA7S8/SbdDj6W5jdfpXlKEri7Dt+Xlc/8vcaVtWppaW7hrvOu48Trz6GhsYGJtz3CnFdncsAPj2TmC6/x0gOT2Ov4Axm69ydobmpi2eL33QWyuur8AqNK3OFYM++efmh9Vsxq6vw7e9e6ClaHLnr95vaedNch7593VNkxZ73Rt6zx/jrK46zNzKDuL9g7WJuZQd13gzhYm5lR/0P3HKzNzMAtazOzTHCwNjPLgHX0dnMzs0zpxN9grAgHazMzcDeImVkmeDSImVkGuGVtZpYBDtZmZvUvmt0NYmZW/9yyNjOrfx66Z2aWBQ7WZmYZUN9d1g7WZmYA0VTf0drB2swM3LI2M8sCX2A0M8sCt6zNzOqfW9ZmZlnglrWZWf2LplrXoDgHazMzIOq8Zd1Q6wqYmdWFlg5MJUg6SNLLkqZJOrtAnq9JmippiqSbSpXplrWZGZ3XspbUCFwBHADMBCZKGhsRU3PyDAXOAfaOiIWSNi5VbsFgLalvsQ0jYkm5lTczq3ed2A0yEpgWEdMBJN0CHAZMzcnzHeCKiFgIEBFzSxVarGU9BQhAOetalwPYvCO1NzOrZ9Gs0plSkkYBo3JWjYmIMen8IGBGTtpMYPe8IrZNy3kcaAR+FhH3FttnwWAdEUPKrLeZWeZ1pGWdBuYxBZLbi/r5g7i7AEOB/YDBwN8l7RQRiwrts6wLjJKOkvTv6fxgSbuVs52ZWVZEi8qeSpgJ5DZ2BwOz2slzV0SsjIjXgJdJgndBJYO1pMuBzwDfSFctBa4qtZ2ZWZZES/lTCROBoZK2ktQNOAoYm5fnTpK4iqQNSbpFphcrtJzRIHtFxHBJzwJExIK0AmZma42I8vusi5cTTZJOAe4j6Y++NiKmSBoNPB0RY9O0AyVNBZqBsyJifrFyywnWKyU1kPa5SBpA3d+YaWbWMZ15U0xEjAPG5a07L2c+gDPSqSzlBOsrgD8BG0n6OfA14Ofl7sDMLAtaOjAapBZKBuuIuF7SJGD/dNVXI+LFylbLzKy6yrhwWFPl3sHYCKwk6QrxLepmttap92BdzmiQnwA3AwNJhqDcJOmcSlfMzKyaIsqfaqGclvVxwG4RsRRA0gXAJODCSlbMzKya6r1lXU6wfiMvXxdKjAc0M8uazhq6VynFHuR0KUkf9VJgiqT70uUDgceqUz0zs+pozvBokNYRH1OAv+Wsf6Jy1TEzq43Mtqwj4ppqVsTMrJYy32ctaWvgAmAHoEfr+ojYtoL1MjOrqlqN8ihXOWOmrwP+h+SxfwcDtwG3VLBOZmZV14lP3auIcoJ1r4i4DyAi/hUR55I+LcrMbG3R3NJQ9lQL5QzdWyFJwL8knQy8BZT8vTAzsyyp926QcoL1D4HewGkkfdf9gG9XslJmZtXWktXRIK0i4sl09l0+/AECM7O1SmaH7kn6Cx/93bBVIuKIitTIzKwGstwNcnnVatGOe2/tW8vdW506dfPZta6CraUy2w0SEQ9WsyJmZrVUq1Ee5Sr3edZmZmu1Ou8FcbA2M4MMd4Pkk9Q9IlZUsjJmZrVS76NByvmlmJGSXgBeTZd3kfSfFa+ZmVkVtXRgqoVyetQvA74IzAeIiMn4dnMzW8sEKnuqhXK6QRoi4o3kjvNVmitUHzOzmmiq826QcoL1DEkjgZDUCJwKvFLZapmZVVetWszlKidY/xtJV8jmwBzggXSdmdlao1Z90eUq59kgc4GjqlAXM7OayXzLWtLVtDNePCJGVaRGZmY1kPmWNUm3R6sewJeBGZWpjplZbTRnvWUdEbfmLku6ARhfsRqZmdVAnf9e7mrdbr4VsEVnV8TMrJZast6ylrSQD/usG4AFwNmVrJSZWbVl+kFO6W8v7kLyu4sALRH1/ohuM7OOq/cLjEVvN08D818iojmdHKjNbK3UIpU91UI5zwZ5StLwitfEzKyGmjsw1ULBYC2ptYvkUyQB+2VJz0h6VtIz1amemVl1tKj8qRRJB6Uxc5qkgtf4JB0pKSSNKFVmsT7rp4DhwOGlq2Zmlm2dNRokfYbSFcABwExgoqSxETE1L18f4DTgyXLKLRasBRAR/1qtGpuZZUgnXpAbCUyLiOkAkm4BDgOm5uX7BfBr4MxyCi0WrDeSdEahxIi4pJwdmJllQUduipE0Csh95MaYiBiTzg+i7V3eM4Hd87YfBgyJiLslrXGwbgR6Q52PFDcz6wQdGbqXBuYxBZLbi5mrGu6SGoBLgRM6sMuiwXp2RIzuSGFmZlnV3HnN0pnAkJzlwcCsnOU+wE7AI+mPumwKjJX0pYh4ulChJfuszczWBZ14U8xEYKikrUhuKDwKOKY1MSIWAxu2Lkt6BDizWKCG4uOsP7cmtTUzy5LO+sHciGgCTgHuA14CbouIKZJGS/rS6tavYMs6IhasbqFmZlnTmT/BGBHjgHF5684rkHe/cspcnafumZmtder92SAO1mZm1O428nI5WJuZsXb++ICZ2VrH3SBmZhngYG1mlgH1/rB+B2szM9xnbWaWCR4NYmaWAS113hHiYG1mhi8wmpllQn23qx2szcwAt6zNzDKhSfXdtnawNjPD3SBmZpngbhAzswzw0D0zswyo71DtYG1mBrgbxMwsE5rrvG3tYG1mhlvWZmaZEG5Zm5nVP7esrahNPrMzw0Z/AzU2MP2mR3j58r+2m2/QISPZ6/c/4IGDzmXh5NfYeN+d2PknR9HQtQstK5uYPPom3nl8anUrbxXTfY9Psv4Zp6CGBt4fO453r7+5TXqvQz5Pv1O/S/M78wB47/Y7WTp2HAD9ThlFj733AInlT01i8SWXV73+WeShe1ZYgxj+yxN49OsXsnT2Ava/5xfMuv8Z3n3lrTbZuqzXg6EnfZ75k6atWvfBgnd57JsXs3zOIvp+fDD73vxj7h5+arVfgVVCQwMbnPUD3jn1LJrnvsPG113Jsr//g6bX3miTbdkDj7Do4svarOv2iR3ptvNOzDn2JAA2GvM7ug/fhRXPTK5a9bOqvkM1NNS6Auuy/sO25r3X5/D+m+8QK5uZcdcTDPr8bh/Jt+OPj+TlK+6mecUHq9YtevENls9ZBMCSl2fS0L0rDd383bs26LbDdjTNfIvmWbOhqYll4x+i5757lbdxBOreDbp2QV27oi5daF6wsLIVXks0EWVPteBgXUM9N+3P0rfmr1peOnsBPTfdoE2e9Xfagl4DBzD7gWcLljPokJEsevENWj5oqlhdrXoaN96Q5jlzVy03z51H40YbfSRfz8/sw8Z/vJr+F/6Uxo2T9A9enMqKSc8x8G93sNm421n+xESaXn+zanXPsujAv1qoerCW9K0iaaMkPS3p6QeWTiuUba2hdn7zLSLaZNjl58cx+Wc3Fiyj77aD2Pnco5j0/66pQA2tNto9MdosLv/7BGYffgxzj/sOK556hg1+ejYAjYMH0mXLzZl96NeY/cWv0X3EMLrtunM1Kp15LR2YaqEWLeufF0qIiDERMSIiRuzfa5tq1qkmls5eQK9BA1Yt99qs/6quDYAuvXvQb7sh7Pfnc/nCU79lwPBt2Pu6H7HBLlsB0HOz/ux17Q956rSreP+NuR8p37Kpee47NG6y8arlxo03pHnevDZ5WpYsgZUrAXj/rr/RbbuhAPTcbx8+eHEqsWw5sWw5yyc8Rbedtq9e5TNsnWxZS3q+wPQCsEkl9plFC5+bTu+tNqXXkI1Q10aGHLYHs+6btCq96d1ljN3xZMaNPJ1xI09n/jPTePyE37Bw8mt07duLT91wJi9ceCvzJ75Sw1dhne2Dl/5JlyGDaNxsU+jShZ4HfJZlj05ok6dhQP9V8z322YuVaVdH89tz6D5sF2hsgMZGug/bxd0gZar3lnWlrkhtAnweyL+yIeAfFdpn5kRzC8/++3Xse/OPUWMDr93yvyx55S12POsrLJj8GrPvf6bgttt8+0B6b7UJO5z+ZXY4/csAPHrUr1gxf0m1qm+V0tzCoov/kw0vuwg1NPL+X++h6bXX6TvqBD546RWW//0f9P76EfTcZy+iuZmWJUtYOPoiAJY99CjdRwxjkxuvAYLlEyay/LEJxfdnADRHfY8HUVSggpKuAf4nIh5rJ+2miDimVBm3b3ZsfR85q4k9N59d6ypYHRr85EPtdPR3zDFbfLnsmHPTG39Z4/11VEVa1hFxYpG0koHazKzafLu5mVkG+HZzM7MMqPfbzX1TjJkZnTt0T9JBkl6WNE3S2e2knyFpajpK7kFJW5Qq08HazIxkNEi5UzGSGoErgIOBHYCjJe2Ql+1ZYERE7AzcAfy6VP0crM3MSLpByp1KGAlMi4jpEfEBcAtwWG6GiHg4Ipami08Ag0sV6mBtZkbHborJfTRGOo3KKWoQMCNneWa6rpATgXtK1c8XGM3M6NjQvYgYA4wpkNzeGOx2C5d0HDAC+HSpfTpYm5nRqaNBZgJDcpYHA7PyM0naH/gJ8OmIWFGqUAdrMzPynni5ZiYCQyVtBbwFHAW0uRlQ0jDgv4GDIqKsp7A5WJuZAc2d1LKOiCZJpwD3AY3AtRExRdJo4OmIGAv8f6A3cLuSZyW/GRFfKlaug7WZGZ17U0xEjAPG5a07L2d+/46W6WBtZkandoNUhIO1mRn1f7u5g7WZGX7qnplZJtT7jw84WJuZ4W4QM7NMcLA2M8sAjwYxM8sAt6zNzDLAo0HMzDKgOer7VxgdrM3McJ+1mVkmuM/azCwD3GdtZpYBLe4GMTOrf25Zm5llgEeDmJllgLtBzMwywN0gZmYZ4Ja1mVkGuGVtZpYBzdFc6yoU5WBtZoZvNzczywTfbm5mlgFuWZuZZYBHg5iZZYBHg5iZZYBvNzczywD3WZuZZYD7rM3MMsAtazOzDPA4azOzDHDL2swsAzwaxMwsA3yB0cwsA+q9G6Sh1hUwM6sH0YF/pUg6SNLLkqZJOrud9O6Sbk3Tn5S0ZakyHazNzEha1uVOxUhqBK4ADgZ2AI6WtENethOBhRGxDXApcFGp+jlYm5mR9FmXO5UwEpgWEdMj4gPgFuCwvDyHAX9I5+8APidJxQqt2z7rr86+sWjF1yWSRkXEmFrXw+qLz4vO1fTBW2XHHEmjgFE5q8bkvBeDgBk5aTOB3fOKWJUnIpokLQYGAPMK7dMt62wYVTqLrYN8XtRIRIyJiBE5U+6XZntBP785Xk6eNhyszcw610xgSM7yYGBWoTySugD9gAXFCnWwNjPrXBOBoZK2ktQNOAoYm5dnLHB8On8k8FCUuHJZt33W1ob7Ja09Pi/qUNoHfQpwH9AdAvdqAAAD7klEQVQIXBsRUySNBp6OiLHANcANkqaRtKiPKlWu6n0guJmZuRvEzCwTHKzNzDLAwbrOlbpt1dY9kq6VNFfSi7Wui1WPg3UdK/O2VVv3XAccVOtKWHU5WNe3cm5btXVMRDxKiTG5tvZxsK5v7d22OqhGdTGzGnKwrm8dviXVzNZODtb1rZzbVs1sHeBgXd/KuW3VzNYBDtZ1LCKagNbbVl8CbouIKbWtldWapJuBCcDHJc2UdGKt62SV59vNzcwywC1rM7MMcLA2M8sAB2szswxwsDYzywAHazOzDHCwtqqS9F76/0BJd5TIe7qkXh0sfz9Jd5e7Pi/PCZIu7+D+Xpe0YUe2MVsdDta2xtKnA3ZIRMyKiCNLZDsd6FCwNltbOVhbQZK2lPRPSX+Q9LykO1pbummL8jxJjwFflbS1pHslTZL0d0nbpfm2kjRB0kRJv8gr+8V0vlHSxZJeSPdzqqTTgIHAw5IeTvMdmJb1jKTbJfVO1x+U1vMx4IgyXtdISf+Q9Gz6/8dzkoekr+NlST/N2eY4SU9Jek7Sf6/OF5TZmnCwtlI+DoyJiJ2BJcD3ctKWR8SnIuIWkh9vPTUidgPOBP4rzfM74MqI+CTwdoF9jAK2Aoal+7kxIi4jeQ7KZyLiM2lXw7nA/hExHHgaOENSD+Bq4FBgH2DTMl7TP4F9I2IYcB7wy5y0kcCxwK4kX0IjJG0PfB3YOyJ2BZrTPGZV4183t1JmRMTj6fwfgdOAi9PlWwHSFu5ewO3SqgcFdk//3xv4Sjp/A3BRO/vYH7gqvb2eiGjvWc17kPwAw+PpPrqR3HK9HfBaRLya1uWPJMG/mH7AHyQNJXmKYdectPERMT8t68/Ap4AmYDdgYrrvnsDcEvsw61QO1lZK/vMIcpffT/9vABalrc5yysinMvOMj4ij26yUdi1j23y/AB6OiC9L2hJ4JCetvdcr4A8RcU4H92PWadwNYqVsLmnPdP5o4LH8DBGxBHhN0lcBlNglTX6c5GmBULjr4H7gZEld0u37p+vfBfqk808Ae0vaJs3TS9K2JF0aW0naOqeOpfQD3krnT8hLO0BSf0k9gcPT+j8IHClp49b6SdqijP2YdRoHayvlJeB4Sc8D/YErC+Q7FjhR0mRgCh/+/NgPgO9LmkgSJNvze+BN4Pl0+2PS9WOAeyQ9HBHvkATWm9O6PAFsFxHLSbo9/pZeYHyjjNf0a+BCSY8D+RcKHyPprnkO+FNEPB0RU0n6y+9P9z0e2KyM/Zh1Gj91zwpKuwjujoidalwVs3WeW9ZmZhnglrWZWQa4ZW1mlgEO1mZmGeBgbWaWAQ7WZmYZ4GBtZpYB/wdLzL0off4OrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (c) # Your Code goes here:\n",
    "lrm = LogisticRegression(solver=\"lbfgs\")\n",
    "lrm.fit(X_tr_scaled,y_tr)\n",
    "plt.figure()\n",
    "cm3=confusion_matrix(y_tst,lrm.predict(X_tst_scaled))\n",
    "print(\"accuracy of testing data after normalisation: {}\".format(accuracy_score(y_tst,lrm.predict(X_tst_scaled))))\n",
    "cm3_norm = cm3/cm3.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm3_norm,lrm.classes_,\"normalised(scaled to mean 0 and variance 1) data\")\n",
    "lrm.fit(X_tr_trans,y_tr)\n",
    "plt.figure()\n",
    "cm3=confusion_matrix(y_tst,lrm.predict(X_tst_trans))\n",
    "print(\"accuracy of testing data after transformation: {}\".format(accuracy_score(y_tst,lrm.predict(X_tst_scaled))))\n",
    "cm3_norm = cm3/cm3.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm3_norm,lrm.classes_,\"transformed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) ***Your answer goes here:*** \n",
    "both classifer gives better accuracy compared to the logistic regression model learned before data processing. However, doing just normalisation without transformation seems to give slightly better result than the one with transformation. As mentioned in the sklearn documentation, this can be explained by the fact that the non linear transformation distort correlations and distances within and across features, and thus making it harder for the logistic regression to learn the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 --- [18 marks] ==========\n",
    "<a id='question2_4'></a>\n",
    "So far we have used default settings for training the logistic regression classifier. Now we want to optimise the hyperparameters of the classifier, namely the regularisation parameter `C`. We will do this through [K-fold cross-validation](http://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.KFold.html). You should familiarise yourself with the interpretation of the `C` parameter.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Why do we use cross-validation to optimise the hyper-parameters, rather than using the test-set?<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Load the datasets `Images_B_Train.csv` and `Images_B_Test.csv` (this ensures everyone is using the same pre-processed data). Again, extract the relevant columns (`dim1` through `dim500` and the `is_person` class) from each dataset, and store into `X_train`/`X_test` and `y_train`/`y_test` variables.<br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Using Cross-Validation on the **Training** set (a 5-fold split should be sufficient: set `shuffle=True` and `random_state=0`), perform a search for the best value of `C` in the range `1e-5` to `1e5` (*Hint: the KFold [split](http://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold.split) method will come in handy*). Keep track of the validation-set accuracy per-fold for each value of `C` in an array. Think carefully about the best way to cover the search space: i.e. the step-lengths and number of steps.<br>\n",
    "&nbsp;&nbsp;**(d)** [Code] Plot the mean and standard-deviation (across folds) of the accuracy as a function of `C`. *Hint: you may find the matplotlib's [errorbar](https://matplotlib.org/2.2.3/api/_as_gen/matplotlib.pyplot.errorbar.html) function useful. Be careful to use the correct scale on the x-axis.* Using the mean values, report the regularisation parameter with the best accuracy (alongside its accuracy): *N.B. Do not pick the optimal value \"by hand\", instead use an appropriate numpy function*.<br>\n",
    "&nbsp;&nbsp;**(e)** [Text] Comment on the output, especially as regards the effect of the regularisation parameter (you should write between 3 and 4 sentences).<br>\n",
    "&nbsp;&nbsp;**(f)** [Code] By using the optimal value (i.e. the one that yields the highest average K-Fold classification accuracy) train a new `LogisticRegression` classifier and report the classification accuracy on the validation set.\n",
    "\n",
    "**N.B.: Keep track of the KFold object you created as we will keep using it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***\n",
    "because the test set is used to estimate the performance of the classifier on unseen data. So we shouldn't use it for training purposes. Otherwise the result would be an overfit to the testing data, and doesn't represent the true generizaiton error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>is_diningtable</th>\n",
       "      <th>is_dog</th>\n",
       "      <th>is_horse</th>\n",
       "      <th>is_motorbike</th>\n",
       "      <th>is_person</th>\n",
       "      <th>is_pottedplant</th>\n",
       "      <th>is_sheep</th>\n",
       "      <th>is_sofa</th>\n",
       "      <th>is_tvmonitor</th>\n",
       "      <th>imgId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.403604</td>\n",
       "      <td>-0.140766</td>\n",
       "      <td>-0.413521</td>\n",
       "      <td>-0.757947</td>\n",
       "      <td>-0.374578</td>\n",
       "      <td>-0.153590</td>\n",
       "      <td>0.431259</td>\n",
       "      <td>-0.073482</td>\n",
       "      <td>0.780903</td>\n",
       "      <td>1.457391</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008_000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.157257</td>\n",
       "      <td>-0.259540</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>1.241051</td>\n",
       "      <td>1.022402</td>\n",
       "      <td>1.641367</td>\n",
       "      <td>1.070421</td>\n",
       "      <td>-0.034985</td>\n",
       "      <td>-0.068578</td>\n",
       "      <td>-0.340810</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008_000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.030825</td>\n",
       "      <td>-0.537627</td>\n",
       "      <td>-1.169096</td>\n",
       "      <td>-0.604982</td>\n",
       "      <td>-0.444052</td>\n",
       "      <td>2.015293</td>\n",
       "      <td>-0.569866</td>\n",
       "      <td>-0.124698</td>\n",
       "      <td>0.790545</td>\n",
       "      <td>1.057791</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008_000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.027341</td>\n",
       "      <td>1.049815</td>\n",
       "      <td>0.039824</td>\n",
       "      <td>-1.104918</td>\n",
       "      <td>-1.189483</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>-1.166647</td>\n",
       "      <td>0.437984</td>\n",
       "      <td>-1.059640</td>\n",
       "      <td>-0.220930</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008_000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.328282</td>\n",
       "      <td>-0.295812</td>\n",
       "      <td>-0.801058</td>\n",
       "      <td>-1.128795</td>\n",
       "      <td>-0.571733</td>\n",
       "      <td>-0.104767</td>\n",
       "      <td>1.557238</td>\n",
       "      <td>-0.840681</td>\n",
       "      <td>0.664188</td>\n",
       "      <td>0.279645</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008_000028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim1      dim2      dim3      dim4      dim5      dim6      dim7  \\\n",
       "0  0.403604 -0.140766 -0.413521 -0.757947 -0.374578 -0.153590  0.431259   \n",
       "1 -0.157257 -0.259540  0.840896  1.241051  1.022402  1.641367  1.070421   \n",
       "2 -1.030825 -0.537627 -1.169096 -0.604982 -0.444052  2.015293 -0.569866   \n",
       "3  1.027341  1.049815  0.039824 -1.104918 -1.189483  0.006663 -1.166647   \n",
       "4 -0.328282 -0.295812 -0.801058 -1.128795 -0.571733 -0.104767  1.557238   \n",
       "\n",
       "       dim8      dim9     dim10     ...       is_diningtable  is_dog  \\\n",
       "0 -0.073482  0.780903  1.457391     ...                    0       0   \n",
       "1 -0.034985 -0.068578 -0.340810     ...                    0       0   \n",
       "2 -0.124698  0.790545  1.057791     ...                    0       1   \n",
       "3  0.437984 -1.059640 -0.220930     ...                    0       0   \n",
       "4 -0.840681  0.664188  0.279645     ...                    0       0   \n",
       "\n",
       "   is_horse  is_motorbike  is_person  is_pottedplant  is_sheep  is_sofa  \\\n",
       "0         1             0          1               0         0        0   \n",
       "1         0             0          0               0         0        0   \n",
       "2         0             0          0               0         0        0   \n",
       "3         0             0          1               0         0        0   \n",
       "4         0             0          0               0         0        0   \n",
       "\n",
       "   is_tvmonitor        imgId  \n",
       "0             0  2008_000008  \n",
       "1             0  2008_000015  \n",
       "2             0  2008_000019  \n",
       "3             1  2008_000023  \n",
       "4             0  2008_000028  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X_train is: (2093, 500)\n",
      "the shape of y_train is: (2093,)\n",
      "the shape of X_test is: (1113, 500)\n",
      "the shape of y_test is: (1113,)\n"
     ]
    }
   ],
   "source": [
    "# (b) # Your Code goes here: \n",
    "from sklearn.model_selection import KFold\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_B_Train.csv')\n",
    "B_train = pd.read_csv(data_path,delimiter=',')\n",
    "B_train_selected = B_train.loc[:,\"dim1\":\"dim500\"]\n",
    "B_train_selected[\"is_person\"]=B_train[\"is_person\"]\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_B_Test.csv')\n",
    "B_test = pd.read_csv(data_path,delimiter=',')\n",
    "B_test_selected = B_test.loc[:,\"dim1\":\"dim500\"]\n",
    "B_test_selected[\"is_person\"]=B_test[\"is_person\"]\n",
    "display(B_train.head())\n",
    "X_train =B_train_selected.drop(\"is_person\",axis=1).values\n",
    "y_train =B_train[\"is_person\"]\n",
    "X_test =B_test_selected.drop(\"is_person\",axis=1).values\n",
    "y_test =B_test[\"is_person\"]\n",
    "print(\"the shape of X_train is: {}\".format(X_train.shape))\n",
    "print(\"the shape of y_train is: {}\".format(y_train.shape))\n",
    "print(\"the shape of X_test is: {}\".format(X_test.shape))\n",
    "print(\"the shape of y_test is: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) # Your Code goes here:\n",
    "import math\n",
    "\n",
    "acclist =np.zeros((10,5))\n",
    "clist = [math.pow(10,x) for x in np.arange(-5,5)] \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for i,c in enumerate(clist):\n",
    "    index = 0\n",
    "    for train_index, test_index in kfold.split(X_train):\n",
    "        X_cross,X_valid = X_train[train_index], X_train[test_index]\n",
    "        \n",
    "        y_cross,y_valid = y_train[train_index], y_train[test_index]\n",
    "        lrm = LogisticRegression(penalty = 'l2',C=c,solver=\"lbfgs\")\n",
    "        lrm.fit(X_cross,y_cross)\n",
    "        acclist[i][index]=lrm.score(X_valid,y_valid)\n",
    "        index=index+1\n",
    "    #acclist.append([accuracy_score(y_valid,j.predict(X_valid)) for j in [LogisticRegression(solver=\"lbfgs\",C=x).fit(X_cross,y_cross) for x in clist ]])\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56563246, 0.61336516, 0.54653938, 0.52870813, 0.55980861],\n",
       "       [0.66825776, 0.6849642 , 0.70405728, 0.6507177 , 0.63636364],\n",
       "       [0.67303103, 0.74224344, 0.73031026, 0.6937799 , 0.65789474],\n",
       "       [0.66825776, 0.71360382, 0.69451074, 0.69617225, 0.66985646],\n",
       "       [0.65871122, 0.65871122, 0.65155131, 0.68181818, 0.65550239],\n",
       "       [0.64200477, 0.64200477, 0.6300716 , 0.6722488 , 0.64832536],\n",
       "       [0.64439141, 0.6372315 , 0.62052506, 0.66267943, 0.6507177 ],\n",
       "       [0.64439141, 0.63484487, 0.61813842, 0.66267943, 0.64593301],\n",
       "       [0.64439141, 0.6372315 , 0.62052506, 0.66267943, 0.64832536],\n",
       "       [0.64439141, 0.6372315 , 0.62052506, 0.66267943, 0.64832536]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00080009 0.00057674 0.00104812 0.0002965  0.00011261 0.00019514\n",
      " 0.00019742 0.00021324 0.00019106 0.00019106]\n",
      "the the regularisation parameter C=0.001 achieves the best accuracy of 0.6994518733370636\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (d) # Your Code goes here:\n",
    "means = np.mean(acclist,axis=1)\n",
    "variances=np.var(acclist,axis=1)\n",
    "print(variances)\n",
    "plt.title(\"error bar of mean and variance of diferent C across fold\")\n",
    "plt.errorbar(x=np.arange(10),y=means,yerr=variances)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"variance of diferent regularisation parameter C across fold\")\n",
    "plt.plot(np.arange(10),variances)\n",
    "best_c = clist[np.argmax(means)]\n",
    "print(\"the the regularisation parameter C={} achieves the best accuracy of {}\".format(best_c,np.max(means)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) ***Your answer goes here:***\n",
    "add the regularization parameter has a positive effect on the predicted accuracy, even if C is very small, the result is still better than the baseline. the accuracy continues to go up rapidly to 0.7 until C reaches somewhere around 0.001 and then goes done and converge to around 0.64. so we see that a balance between maximising log possibility and minimising mis classification can produce the best outcome, but even if the regularization parameter becomes the dominant factor of the classification the result is still better than baseline. And also the variance is at its peak when the accuracy is maximised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (f) # Your Code goes here:\n",
    "lrm = LogisticRegression(penalty = 'l2',C=c,solver=\"lbfgs\")\n",
    "lrm.fit(X_train,y_train)\n",
    "print(\"the classification accuracy on test set is {}\".format(lrm.score(X_train,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.5 --- (LEVEL 11) --- [12 marks] ==========\n",
    "\n",
    "Let us attempt to validate the importance of the various features for classification. We could do this like we did for linear regression by looking at the magnitude of the weights. However, in this case, we will use the [`RandomForestClassifier`](http://scikit-learn.org/0.19/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to give us a ranking over features.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] How can we use the Random-Forest to get this kind of analysis? *Hint: look at the `feature_importances` property in the SKLearn implementation.*<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Initialise a random forest classifier and fit the model by using training data only and 500 trees (i.e. `n_estimators=500`). Set `random_state=42` to ensure reproducible results and `criterion=entropy` but leave all other parameters at their default value. Report the accuracy score on both the training and testing sets.<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Comment on the discrepancy between training and testing accuracies.<br>\n",
    "&nbsp;&nbsp;**(d)** [Code] By using the random forest model display the names of the 10 most important features (in descending order of importance).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) # Your Code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.6 --- [12 marks] ==========\n",
    "\n",
    "We would like now to explore another form of classifier: the Support Vector Machine. A key decision in training SVM's is what kind of kernel to use. We will explore with three kernel types: linear, radial-basis-functions and polynomials. To get a feel for each we will first visualise typical decision boundaries for each of these variants. To do so, we have to simplify our problem to two-dimensional input (to allow us to visualise it).\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Using the training set only, create a training `X` matrix with only the `dim21` and `dim51` columns. ***N.B.*** *Python (and numpy) use zero-based indexing*. Then train three distinct classifiers on this 2D data. Use a `linear` kernel for one, an `rbf` kernel for another (set `gamma='auto'`) and a second order (`degree`) polynomial kernel for the other. Set `C=1` in all cases. Using the function `plot_SVM_DecisionBoundary` from our own library (it exists under the `plotters` module), plot the decision boundary for all three classifiers.<br>\n",
    "&nbsp;&nbsp;**(b)** [Text] Explain (intuitively) the shape of the decision boundary for each classifier (i.e. comment on what aspect of the kernel gives rise to it). Use this to comment on how it relates to classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_train.select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s16/s1603859/miniconda3/envs/py3iaml/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim21</th>\n",
       "      <th>dim51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135315</td>\n",
       "      <td>0.048255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044829</td>\n",
       "      <td>-0.118876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.660638</td>\n",
       "      <td>-0.352501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.822104</td>\n",
       "      <td>-0.620271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-0.474705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.023666</td>\n",
       "      <td>-0.843113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.488877</td>\n",
       "      <td>-0.882051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.771714</td>\n",
       "      <td>-0.787403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.246933</td>\n",
       "      <td>-0.678378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.305195</td>\n",
       "      <td>-0.067958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.535412</td>\n",
       "      <td>1.763301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.308926</td>\n",
       "      <td>-0.015243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.505611</td>\n",
       "      <td>-0.232094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.225228</td>\n",
       "      <td>-0.882051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-0.678378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.167507</td>\n",
       "      <td>-0.882051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.857323</td>\n",
       "      <td>-1.085124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.857323</td>\n",
       "      <td>1.356554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-0.678378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.799061</td>\n",
       "      <td>1.966974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.620542</td>\n",
       "      <td>-0.397429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.698822</td>\n",
       "      <td>0.413069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-1.288797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.305195</td>\n",
       "      <td>-1.085124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.488877</td>\n",
       "      <td>-0.271631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.063251</td>\n",
       "      <td>1.356554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.799061</td>\n",
       "      <td>0.135715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.556606</td>\n",
       "      <td>-1.042593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.225228</td>\n",
       "      <td>-1.288797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.015856</td>\n",
       "      <td>1.162466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>-0.015856</td>\n",
       "      <td>-0.843113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2.584401</td>\n",
       "      <td>0.583196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>-0.857323</td>\n",
       "      <td>0.746135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>0.246933</td>\n",
       "      <td>1.152881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-0.474705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>-0.519761</td>\n",
       "      <td>-0.898824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>-0.096589</td>\n",
       "      <td>-0.352501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>0.246933</td>\n",
       "      <td>-0.067958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-0.678378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>-1.032877</td>\n",
       "      <td>-1.288797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>-0.488877</td>\n",
       "      <td>-1.288797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>0.246933</td>\n",
       "      <td>2.373721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>-0.857323</td>\n",
       "      <td>-0.678378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>-0.620542</td>\n",
       "      <td>-0.843113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>-1.041005</td>\n",
       "      <td>-0.678378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>0.063251</td>\n",
       "      <td>-0.271631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>-1.041005</td>\n",
       "      <td>-1.085124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>-1.023666</td>\n",
       "      <td>-1.288797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>-0.673100</td>\n",
       "      <td>-0.474705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>0.185706</td>\n",
       "      <td>-0.397429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>-1.225228</td>\n",
       "      <td>-1.288797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>-0.305195</td>\n",
       "      <td>-0.474705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>-1.023666</td>\n",
       "      <td>0.271097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>1.167507</td>\n",
       "      <td>0.542462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>0.063251</td>\n",
       "      <td>-1.085124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>-1.032877</td>\n",
       "      <td>-0.650822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>-0.418980</td>\n",
       "      <td>0.271097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>-0.857323</td>\n",
       "      <td>-0.271631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>0.185706</td>\n",
       "      <td>0.271097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>0.063251</td>\n",
       "      <td>-1.085124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dim21     dim51\n",
       "0     0.135315  0.048255\n",
       "1     0.044829 -0.118876\n",
       "2    -0.660638 -0.352501\n",
       "3    -0.822104 -0.620271\n",
       "4    -0.673100 -0.474705\n",
       "5    -1.023666 -0.843113\n",
       "6    -0.488877 -0.882051\n",
       "7    -0.771714 -0.787403\n",
       "8     0.246933 -0.678378\n",
       "9    -0.305195 -0.067958\n",
       "10    1.535412  1.763301\n",
       "11    1.308926 -0.015243\n",
       "12    1.505611 -0.232094\n",
       "13   -1.225228 -0.882051\n",
       "14   -0.673100 -0.678378\n",
       "15    1.167507 -0.882051\n",
       "16   -0.857323 -1.085124\n",
       "17   -0.857323  1.356554\n",
       "18   -0.673100 -0.678378\n",
       "19    0.799061  1.966974\n",
       "20   -0.620542 -0.397429\n",
       "21    0.698822  0.413069\n",
       "22   -0.673100 -1.288797\n",
       "23   -0.305195 -1.085124\n",
       "24   -0.488877 -0.271631\n",
       "25    0.063251  1.356554\n",
       "26    0.799061  0.135715\n",
       "27   -0.556606 -1.042593\n",
       "28   -1.225228 -1.288797\n",
       "29   -0.015856  1.162466\n",
       "...        ...       ...\n",
       "2063 -0.015856 -0.843113\n",
       "2064  2.584401  0.583196\n",
       "2065 -0.857323  0.746135\n",
       "2066  0.246933  1.152881\n",
       "2067 -0.673100 -0.474705\n",
       "2068 -0.519761 -0.898824\n",
       "2069 -0.096589 -0.352501\n",
       "2070  0.246933 -0.067958\n",
       "2071 -0.673100 -0.678378\n",
       "2072 -1.032877 -1.288797\n",
       "2073 -0.488877 -1.288797\n",
       "2074  0.246933  2.373721\n",
       "2075 -0.857323 -0.678378\n",
       "2076 -0.620542 -0.843113\n",
       "2077 -1.041005 -0.678378\n",
       "2078  0.063251 -0.271631\n",
       "2079 -1.041005 -1.085124\n",
       "2080 -1.023666 -1.288797\n",
       "2081 -0.673100 -0.474705\n",
       "2082  0.185706 -0.397429\n",
       "2083 -1.225228 -1.288797\n",
       "2084 -0.305195 -0.474705\n",
       "2085 -1.023666  0.271097\n",
       "2086  1.167507  0.542462\n",
       "2087  0.063251 -1.085124\n",
       "2088 -1.032877 -0.650822\n",
       "2089 -0.418980  0.271097\n",
       "2090 -0.857323 -0.271631\n",
       "2091  0.185706  0.271097\n",
       "2092  0.063251 -1.085124\n",
       "\n",
       "[2093 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (a) # Your Code goes here:\n",
    "classifiers = []\n",
    "from utils.plotter import plot_SVM_DecisionBoundary\n",
    "X = B_train.select(lambda x:x==\"dim21\" or x==\"dim51\",axis=1)\n",
    "X=X.values\n",
    "svc_linear = SVM(C=1,kernel=\"linear\")\n",
    "svc_linear.fit(X, y_train)\n",
    "svc_linear = SVM(C=1,kernel=\"linear\")\n",
    "svc_linear.fit(X, y_train)\n",
    "svc_linear = SVM(C=1,kernel=\"linear\")\n",
    "svc_linear.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.7 --- [14 marks] ==========\n",
    "Let us now explore the polynomial SVM further. We will go back to using the **FULL** dataset (i.e. the one we loaded in [Question 2.4](#question2_4)). There are two parameters we need to tune: the order of the polynomial and the regression coefficient. We will do this by way of a grid-search over parameters. To save computational time, we will use a constrained search space:\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Define an appropriate search space for `C` in the range `1e-2` to `1e3` using 6-steps (think about the step-size), and for the `degree` in the range 1 through 5 inclusive (5 steps). Using the `K-fold` iterator from [Q2.5](#question2_4), optimise the values for `C` and the `degree` in the above specified range. Keep track of the mean cross-validation accuracy for each parameter combination.<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using a seaborn heatmap, plot the fold-averaged classification accuracy for each parameter combination (label axes appropriately). Finally also report the combination of the parameters which yielded the best accuracy.<br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Retrain the (polynomial-kernel) SVC using the optimal parameters found in **(b)** and report its accuracy on the **Testing** set.<br>\n",
    "&nbsp;&nbsp;**(d)** [Text] Explain the results relative to the Logistic Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.8 --- (LEVEL 11) --- [10 marks] ==========\n",
    "\n",
    "Answer the followign theoretical questions:\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Is a Logistic Regression Classifier equivalent to an SVM with a Linear Kernel? why or why not?<br>\n",
    "&nbsp;&nbsp;**(b)** [Text] In the previous question we optimised the `degree` and regularisation `C` simultaneously. By looking at the heatmap you plotted, can you explain the motivation behind this? That is, what would happen if we were to estimate the optimum along each dimension independently? Can you imagine a case where an independent search along each of the dimensions (known as coordinate-descent) would be guaranteed to yield the optimum result?<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Despite having a hold-out testing set, we used cross-validation for optimising the hyper-parameters (only using the testing set for comparing models). Why is this? Also, mention an advantage and a disadvantage of using cross-validation to train hyper-parameters rather than a further train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## More information about visual words\n",
    "\n",
    "The Visual words used in this project are based on [Scale-invariant feature transforms (SIFT)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). SIFT features are essentially local orientation histograms and capture the properties of small image regions. They possess attractive invariance properties which make them well suited for our task (you can read more about SIFT features in [D.Lowe, IJCV 60(2):91- 110, 2004](http://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94), but the details don't matter for the purpose of this assignment). Each SIFT feature is a 128 dimensional vector. From each image many SIFT features are extracted, typically > 2500 per image (features are extracted at regular intervals using a 15 pixel grid and at 4 different scales). To obtain visual words a representative subset of all extracted SIFT features from all images is chosen and clustered with k-means using 500 centres (such use of the k-means algorithm will be discussed in detail during the lecture). These 500 cluster centres form our visual words. The representation of a single image is obtained by first assigning each SIFT feature extracted from the image to the appropriate cluster (i.e. we determine the visual word corresponding to each feature by picking the closest cluster centre). We then count the number of features from that image assigned to each cluster (i.e. we determine how often each visual word is present in the image). This results in a 500 dimensional count vector for each image (one dimension for each visual word). The normalized version of this count vector gives the final representation of the image (normalized means that we divide the count vector by the total number of visual words in the image, i.e. the normalized counts sum to 1 for each image)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
